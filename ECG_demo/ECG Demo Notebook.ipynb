{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"report-header\"><div class=\"aictx-logo\"></div>\n",
    "<span class=\"report-type\">Demonstration</span><br />\n",
    "<span class=\"report-author\">Author: Felix Bauer</span><br />\n",
    "<span class=\"report-date\">25th January, 2020</span>\n",
    "</div><h1>Live Demo:</h1><h1>ECG anomaly detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how recurrent SNNs can be used for anomaly detection in an electrocardiogram (ECG) signal. The main part of the network will run on a DYNAP-SE neuromorphic processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /home/felix/miniconda3/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      " [py.warnings]\n"
     ]
    }
   ],
   "source": [
    "### --- Imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dataloader import ECGDataLoader\n",
    "from rockpool.layers import (\n",
    "    FFUpDown,\n",
    "    FFIAFSpkInRefrTorch,\n",
    "    RecIAFSpkInNest,\n",
    "    RecDynapSE,\n",
    "    FFExpSyn,\n",
    ")\n",
    "from rockpool import Network\n",
    "\n",
    "### --- Constants\n",
    "DT_ECG = 0.002778\n",
    "NUM_ECG_LEADS = 2\n",
    "NUM_ANOM_CLASSES = 5\n",
    "MAX_FANIN = 64  # Max. number of presynaptic connections per neuron (hardware limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Our goal is to detect five different classes of anomalies in a two-lead ECG signal from the MIT-BIH Arrithmia Database [ref]. An SNN that fulfills this task can, for instance, be used in a wearable ECG monitoring device to trigger an alarm in presence of pathological patterns. Below we see examples for a normal ECG signal and for each anomaly type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from plot_example_beats import plot_examples\n",
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware\n",
    "\n",
    "We will first run a software simulation of our SNN and then run the network directly on a DYNAP-SE neuromorphic processor. The device we are demonstrating here imposes a few restrictions on the network which are described below and which will also be considered in the software simulations.\n",
    "\n",
    "## Core-wise parameters\n",
    "Neuron and synapse parameters, such as time constants, firing thresholds and weights are set per core. The present processor consists of 16 cores of 256 neurons each. \n",
    "\n",
    "## Discrete weights\n",
    "Synaptic weights are the same for each postsynaptic neuron on a core, resulting in ternary weights: positive (excitatory), negative (inhibitory), and zero (not connected). However, between each pair of neurons, multiple connections are possible, which effectively allows for integer weights.\n",
    "\n",
    "## Connectivity\n",
    "The number of presynaptic connections to each neuron is generally limited to 64.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal-to-spike encoding\n",
    "\n",
    "The analog ECG signal is converted to trains of events through a sigma-delta encoding scheme. For every ECG lead there are two output channels, emitting spikes when the input signal increases (\"up\"-channel) or decreases (\"down\"-channel) by a specified amount.\n",
    "\n",
    "The four resulting spike trains serve as input to the acutal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Spike encoding\n",
    "spike_enc = FFUpDown(\n",
    "    weights=NUM_ECG_LEADS,\n",
    "    dt=DT_ECG,    \n",
    "    thr_up=0.1,\n",
    "    thr_down=0.1,\n",
    "    multiplex_spikes=True,\n",
    "    name=\"spike encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture\n",
    "\n",
    "\n",
    "To detect the anomalies we will use a reservoir network consisting of three layers:\n",
    "\n",
    "## Input expansion layer\n",
    "\n",
    "This layer consists of 128 neurons, each of which with up to 64 presynaptic excitatory connections (or a positive integer weight up to 64) to one of the four input channels. This connection scheme ensures that the neurons respond differently to the input, therefore increasing the dimensionality of the signal. \n",
    "\n",
    "This is enhanced by the fact that the hardware neurons on the DYNAP-SE slightly vary in their individual characteristics, which will result in richer neuron dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# - Input expansion layer\n",
    "num_ch_in = 2 * NUM_ECG_LEADS\n",
    "size_expand = 128\n",
    "baseweight_expand = 5e-4\n",
    "\n",
    "# weights_expand = np.zeros((num_ch_in, size_expand))\n",
    "# num_input_conns = np.random.randint(1, MAX_FANIN + 1, size=size_expand)\n",
    "# num_neur_per_ext = int(np.floor(size_expand / num_ch_in))\n",
    "# for idx_ch_in in range(num_ch_in):\n",
    "#     idcs_inp = slice(idx_ch_in * num_neur_per_ext, (idx_ch_in + 1) * num_neur_per_ext)\n",
    "#     weights_expand[idx_ch_in, idcs_inp] = num_input_conns[idcs_inp]\n",
    "    \n",
    "weights_expand = np.load(\"weights_expand.npy\")\n",
    "\n",
    "plt.imshow(weights_expand, aspect=\"auto\")\n",
    "plt.title(\"Weights of dimensionality expansion layer\", fontsize=18)\n",
    "\n",
    "# Scale weights for software simulation\n",
    "weights_expand_scaled = weights_expand * baseweight_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer `input_expansion_layer`: Using CPU as CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "# Load parameters from file (generated with gen_params.py)\n",
    "kwargs_expand = dict(np.load(\"kwargs_expand.npz\"))\n",
    "\n",
    "# - Instantiate layer object\n",
    "inp_expansion = FFIAFSpkInRefrTorch(\n",
    "    weights=weights_expand_scaled,\n",
    "    name=\"input_expansion_layer\",\n",
    "    **kwargs_expand\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservoir layer\n",
    "\n",
    "The reservoir layer consists of 512 excitatory and neuron 128 inhibitory neurons which are connected recurrently in a stochastic manner. Due to the recurrent connections, the network state at a given time does not only depend on the current input but also on previous network states, therefore implicitly encoding information about past inputs. This makes it possible to processs temporal relations in the input signal.\n",
    "\n",
    "As a result, the state of each neuron in the reservoir is a function of the history of the input signal. If the functions ... by all neurons together are linearly independent (have high dimensionality) ... can combine them to approximate arbitrary functions. This linear combniation is done by the readout layer.\n",
    "\n",
    "The separation into excitatory and inhibitory neurons is not strictly required but makes it easier to control neuron dynamics on the neuromorhpic processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All neurons have exactly 64 presynaptic connections.\n"
     ]
    }
   ],
   "source": [
    "## -- Reservoir layer\n",
    "size_rec_exc = 512\n",
    "size_inh = 128\n",
    "size_reservoir = size_rec_exc + size_inh\n",
    "\n",
    "# - Connections to excitatory layer\n",
    "num_exp_rec = 16\n",
    "num_rec = 32\n",
    "num_inh = 16\n",
    "# - Connections to inhibitory layer\n",
    "num_rec_inh = 64\n",
    "\n",
    "baseweight_exp_rec = 8e-5\n",
    "baseweight_rec = 8e-5  # 1.75e-4\n",
    "baseweight_rec_inh = 8e-5\n",
    "baseweight_inh = 1e-4\n",
    "\n",
    "# # - Input connections to reservoir\n",
    "# weights_res_in = np.zeros((size_expand, size_reservoir))\n",
    "\n",
    "# # Connections only go to excitatory part or reservoir (first `size_rec_exc` neurons)\n",
    "# presyn_conns_in = np.random.randint(size_expand, size=(size_rec_exc, num_exp_rec))\n",
    "# for i_post, pre_neurs in enumerate(presyn_conns_in):\n",
    "#     for i_pre in pre_neurs:\n",
    "#         weights_res_in[i_pre, i_post] += 1\n",
    "\n",
    "# # - Recurrent reservoir connections\n",
    "# weights_rec = np.zeros((size_reservoir, size_reservoir))\n",
    "\n",
    "# # Excitatory recurrent connections\n",
    "# presyn_conns_exc = np.random.randint(size_rec_exc, size=(size_rec_exc, num_rec))\n",
    "# for i_post, pre_neurs in enumerate(presyn_conns_exc):\n",
    "#     for i_pre in pre_neurs:\n",
    "#         weights_rec[i_pre, i_post] += 1\n",
    "\n",
    "# # Connections from excitatory to inhibitory population\n",
    "# presyn_conns_exc_inh = np.random.randint(size_rec_exc, size=(size_inh, num_rec_inh))\n",
    "# for i_post, pre_neurs in enumerate(presyn_conns_exc_inh):\n",
    "#     for i_pre in pre_neurs:\n",
    "#         weights_rec[i_pre, i_post + size_rec_exc] += 1\n",
    "\n",
    "# # Inhibitory connections\n",
    "# presyn_conns_inh = np.random.randint(size_inh, size=(size_rec_exc, num_inh))\n",
    "# for i_post, pre_neurs in enumerate(presyn_conns_inh):\n",
    "#     for i_pre in pre_neurs:\n",
    "#         weights_rec[i_pre + size_rec_exc, i_post] += 1\n",
    "        \n",
    "# # - For each neuron count number of presynaptic connections\n",
    "# if (np.sum(weights_res_in, axis=0) + np.sum(weights_rec, axis=0) == MAX_FANIN).all():\n",
    "#     print(f\"All neurons have exactly {MAX_FANIN} presynaptic connections.\")\n",
    "\n",
    "# np.save(\"weights_res_in.npy\", weights_res_in)\n",
    "# np.save(\"weights_rec.npy\", weights_rec)\n",
    "\n",
    "weights_res_in = np.load(\"weights_res_in.npy\")\n",
    "weights_rec = np.load(\"weights_rec.npy\")\n",
    "\n",
    "weights_res_in_scaled = weights_res_in.copy() * baseweight_exp_rec\n",
    "weights_rec_scaled = weights_rec.copy()\n",
    "weights_rec_scaled[:512, :512] *= baseweight_rec\n",
    "weights_rec_scaled[:512, 512:] *= baseweight_rec_inh\n",
    "weights_rec_scaled[512:, :512] *= baseweight_inh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Presynaptic neurons')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.imshow(weights_res_in_scaled, aspect=\"auto\")\n",
    "plt.title(\"Connections from expansion layer to reservoir\")\n",
    "plt.xlabel(\"Postsynaptic neurons\")\n",
    "plt.ylabel(\"Presynaptic neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Presynaptic neurons')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.imshow(weights_rec_scaled, aspect=\"auto\")\n",
    "plt.title(\"Recurrent reservoir connections\")\n",
    "plt.xlabel(\"Postsynaptic neurons\")\n",
    "plt.ylabel(\"Presynaptic neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /home/felix/miniconda3/lib/python3.7/site-packages/nest/lib/hl_api_helper.py:127: UserWarning:\n",
      "ResetNetwork is deprecated and will be removed in NEST 3.0.\n",
      " [py.warnings]\n"
     ]
    }
   ],
   "source": [
    "# - Load reservoir parameters from file (generated with gen_params.py)\n",
    "kwargs_reservoir = dict(np.load(\"kwargs_reservoir.npz\"))\n",
    "\n",
    "# - Instantiate reservoir layer object\n",
    "reservoir = RecIAFSpkInNest(\n",
    "    weights_in=weights_res_in_scaled,\n",
    "    weights_rec=weights_rec_scaled,\n",
    "    name=\"reservoir layer\",\n",
    "    **kwargs_reservoir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout layer\n",
    "- filter reservoir spikes \n",
    "- combine filtered spike trains\n",
    "- use regression to train weights such that output is close to target (1 for anomaly, 0 for normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = FFExpSyn(\n",
    "    weights=np.zeros((size_reservoir, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_net = Network(spike_enc, inp_expansion, reservoir, readout, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "(just load pre-trained data, but show how training would work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG signal and annotaitions have been loaded from /home/felix/gitlab/Projects/AnomalyDetection/ECG/ecg_recordings\n",
      "\n",
      "\tBatch 1 of 1\n",
      "Network: Evolving layer `spike encoder` with external input as input\n",
      "Network: Evolving layer `input_expansion_layer` with spike encoder's output as input\n",
      "TSEvent `Spikes from analogue`: There are channels with multiple events per time step. Consider using a smaller `dt` or setting `add_events = True`.\n",
      "Network: Evolving layer `reservoir layer` with input_expansion_layer's output as input\n",
      "Network: Evolving layer `readout layer` with reservoir layer's output as input\n"
     ]
    }
   ],
   "source": [
    "# - Object to load ECG data\n",
    "data_loader = ECGDataLoader()\n",
    "\n",
    "# - Generator that yields batches of ECG data\n",
    "batchsize_training = 100\n",
    "num_beats = 30\n",
    "regularize = 0.1\n",
    "batch_gen = data_loader.get_batch_generator(num_beats=num_beats, batchsize=batchsize_training)\n",
    "\n",
    "for batch in batch_gen:\n",
    "    output = sw_net.evolve(batch.input)\n",
    "    readout.train_rr(\n",
    "        batch.target,\n",
    "        output[\"reservoir layer\"],\n",
    "        is_first=batch.is_first,\n",
    "        is_last=batch.is_last,\n",
    "        regularize=regularize,\n",
    "    )\n",
    "    \n",
    "sw_net.reset_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "batchsize = 40\n",
    "num_beats = 100\n",
    "ecg_data = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "sw_net.reset_all()\n",
    "inference_out = sw_net.evolve(ecg_data.input)[\"readout layer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "inference_out.clip(channels=0).plot()\n",
    "ecg_data.target.clip(channels=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_net.reset_all()\n",
    "tsr = output[\"reservoir layer\"].delay(-output[\"reservoir layer\"].t_start)\n",
    "tsr\n",
    "readout.reset_all()\n",
    "op = readout.evolve(tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "op.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "op.clip(channels=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Implementation\\\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
