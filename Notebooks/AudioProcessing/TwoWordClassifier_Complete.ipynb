{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDsvtw8mCgDj"
   },
   "source": [
    "# **Towards Wake Word Detection - A Two Word Classifier**\n",
    "\n",
    "Author: Philipp Weidel\n",
    "\n",
    "Wake word detection is the task to detect one specific word in a stream of contiuous speech or noise. Wake word detection has been solved in a satisfactory fashion by Google, Amazon etc. Nowadays, the challange in this task is to solve it consuming less energy and with less delay than state-of-the art solutions.\n",
    "\n",
    "Spiking neural networks have the capability to react much faster than their rate-based counterparts while burning much less energy. This tutorial shows a simplified task towards wake word detection using a spiking neural network in which a classifier is trained to distinguish two words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gNJB8SARtN4S",
    "outputId": "1dd08e6b-c312-4d22-ac04-ba8bc269c810"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# - Disable warning display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from IPython.display import Javascript, Audio, SVG\n",
    "import librosa\n",
    "import librosa.display as rdp\n",
    "from tqdm import tqdm\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "if importlib.util.find_spec('soundfile') is None:\n",
    "  !pip install soundfile\n",
    "import soundfile\n",
    "\n",
    "# install NEST\n",
    "if importlib.util.find_spec('nest') is None:\n",
    "  !wget https://github.com/nest/nest-simulator/archive/v2.18.0.tar.gz\n",
    "  !tar xf v2.18.0.tar.gz\n",
    "  %cd nest-simulator-2.18.0/\n",
    "  !mkdir build && mkdir install\n",
    "  %cd build\n",
    "  !cmake .. -DCMAKE_INSTALL_PREFIX:PATH=../install -Dwith-python=3\n",
    "  !make -j 8 && make install -j 8\n",
    "  %cd ../..\n",
    "  sys.path.append('/content/nest-simulator-2.18.0/install/lib/python3.6/site-packages')\n",
    "import nest\n",
    "\n",
    "# install rockpool\n",
    "if importlib.util.find_spec('rockpool') is None:\n",
    "  !pip install rockpool\n",
    "\n",
    "# download data\n",
    "if not os.path.exists(\"audio_data.tar\"):\n",
    "  !wget https://www.dropbox.com/s/oclm9ahsalbd16s/audio_data.tar\n",
    "  !wget https://www.dropbox.com/s/ewqeun7f8orxm9o/network.svg\n",
    "  !tar xf audio_data.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWPblTqE6fN7"
   },
   "source": [
    "# **Data inspection**\n",
    "\n",
    "The dataset we use here is part of the Google Tensorcommand dataset, which originally contains 30 keywords. For this tutorial we selected 5 keywords (yes, no, left, right and on).\n",
    "\n",
    "First, let's inspect the data at hand. We specify the location of the data in *DATA_PATH* and load an example for each of the keywords **yes** and **no**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpSuKQkFwhTY"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"audio_data/\"\n",
    "\n",
    "sample_yes = \"fac7deca_nohash_0.wav\"\n",
    "sample_no = \"e649aa92_nohash_0.wav\"\n",
    "\n",
    "example_audio_yes = DATA_PATH + f\"audio/yes/{sample_yes}.wav\"\n",
    "example_audio_no = DATA_PATH + f\"audio/no/{sample_no}.wav\"\n",
    "\n",
    "example_yes, sr = librosa.load(example_audio_yes)\n",
    "example_no, sr = librosa.load(example_audio_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "zkuODVFa6d50",
    "outputId": "8980c64e-3a41-4480-c226-ccc769895235"
   },
   "outputs": [],
   "source": [
    "Audio(example_audio_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "LiJea6ym8tNY",
    "outputId": "06ad7fd4-dbf2-4b15-f7f0-e168701d9bda"
   },
   "outputs": [],
   "source": [
    "Audio(example_audio_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "752WGmeu9mmg"
   },
   "source": [
    "## **Visualization**\n",
    "\n",
    "As you heard, the data contains short recordings of multiple speakers saying single words. Also, the quality of recordings is rather poor (which is intended by the creator of the dataset).\n",
    "\n",
    "Now let's visualize the data. We can of course visualize the data as a one dimensional signal of sound-pressure in time-domain.\n",
    "\n",
    "We finde that the samples are each one second long and have a sampling rate of 16kHz. Also, each sample is normalized such that the maximum absolute value is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "lQmFPbJB8xR-",
    "outputId": "9d66850d-eebe-4c95-e269-a0a7a79c7bab"
   },
   "outputs": [],
   "source": [
    "t_example = np.arange(0, len(example_yes) / sr, 1 / sr)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax0.set_title(\"YES\")\n",
    "ax0.set_xlabel(\"time (s)\")\n",
    "ax0.set_ylabel(\"amplitude\")\n",
    "ax0.plot(t_example, example_yes)\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.set_title(\"NO\")\n",
    "ax1.set_xlabel(\"time (s)\")\n",
    "ax1.set_ylabel(\"amplitude\")\n",
    "ax1.plot(t_example, example_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tO73nvc892r1"
   },
   "source": [
    "### **Frequency domain**\n",
    "\n",
    "Audio is better described in frequency domain. So let's see how these two samples compare after transformation using short time fourier transformation (STFT).\n",
    "\n",
    "As the sampling rate is 16kHz, we can see frequency components up to 8kHz (nyquist frequency). The keyword **yes** contains much more high frequency components than **no**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "j7n5hRM79sWt",
    "outputId": "eb51b992-6463-4e89-fd73-ab3b8061d851"
   },
   "outputs": [],
   "source": [
    "stft_yes = librosa.stft(example_yes)\n",
    "stft_no = librosa.stft(example_no)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax0.set_title(\"YES\")\n",
    "rdp.specshow(librosa.amplitude_to_db(stft_yes), sr=sr, x_axis='time', y_axis='hz')\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.set_title(\"NO\")\n",
    "rdp.specshow(librosa.amplitude_to_db(stft_no), sr=sr, x_axis='time', y_axis='hz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9BqULko-HwN"
   },
   "source": [
    "### **Time continuous filter**\n",
    "\n",
    "In order to compute the STFT we have to bin the data into at least 10 ms time bins which contradicts our goal to get a low-latency solution. Also it is difficult to build an STFT in asynchronous neuromorphic hardware. \n",
    "\n",
    "For low-energy and low-latency solutions with very similar results, we can use time-continuous filters such as Butterworh band-pass filters. The butterworth filter removes frequncies in the original data which are outside two cutoff frequncies. After rectification and applying a low-pass filter, we obtain the power over time in specific frequency bands.\n",
    "\n",
    "We provide pre-filtered data using 32 2nd order Butterworth band-pass filters linearly distributed in [Mel](https://en.wikipedia.org/wiki/Mel_scale) scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "lnBO2tc1-CyE",
    "outputId": "50f69409-0992-403c-eecc-51035f1cdc13"
   },
   "outputs": [],
   "source": [
    "example_preprocessed_yes = DATA_PATH + f\"cache/yes/{sample_yes}_filtered.npz\"\n",
    "example_preprocessed_no = DATA_PATH + f\"cache/no/{sample_no}_filtered.npz\"\n",
    "\n",
    "preprocessed_yes = np.load(example_preprocessed_yes, allow_pickle=True)\n",
    "preprocessed_no = np.load(example_preprocessed_no, allow_pickle=True)\n",
    "\n",
    "len_samples = 100\n",
    "preprocessed_sr = 100.\n",
    "num_channels = 32\n",
    "num_labels = 2\n",
    "\n",
    "t_preprocessed = np.arange(0, len(preprocessed_yes) / preprocessed_sr, 1 / preprocessed_sr)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(221)\n",
    "ax0.set_title(\"YES\")\n",
    "ax0.set_ylabel(\"frequency channel\")\n",
    "ax0.set_xticks([])\n",
    "plt.imshow(preprocessed_yes[:, ::-1].T, aspect='auto', cmap='Reds', vmax=0.05)\n",
    "\n",
    "ax1 = fig.add_subplot(222)\n",
    "ax1.set_title(\"NO\")\n",
    "ax1.set_ylabel(\"frequency channel\")\n",
    "ax1.set_xticks([])\n",
    "plt.imshow(preprocessed_no[:, ::-1].T, aspect='auto', cmap='Reds', vmax=0.05)\n",
    "\n",
    "ax2 = fig.add_subplot(223)\n",
    "ax2.set_xlabel(\"time (s)\")\n",
    "ax2.set_ylabel(\"amplitude\")\n",
    "ax2.plot(t_preprocessed, preprocessed_yes)\n",
    "\n",
    "ax3 = fig.add_subplot(224)\n",
    "ax3.set_xlabel(\"time (s)\")\n",
    "ax3.set_ylabel(\"amplitude\")\n",
    "ax3.plot(t_preprocessed, preprocessed_no)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jP5-VqcH-aQ_"
   },
   "source": [
    "# **Training**\n",
    "\n",
    "## **Load more data**\n",
    "\n",
    "Now we have to load more data to train a model.\n",
    "\n",
    "Here you can specify the number of samples to use and also which keywords to use.\n",
    "Available keywords are **yes**, **no**, **left**, **right** and **on**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqCxLOl_-KY8"
   },
   "outputs": [],
   "source": [
    "from glob2 import glob\n",
    "\n",
    "percentage = 0.1 # how much data to use\n",
    "\n",
    "keywords = ['yes', 'no'] # options: left, right, on, yes, no\n",
    "ratio_train = 0.8\n",
    "\n",
    "# get all files using these keywords and randomize order\n",
    "fns = []\n",
    "for keyword in keywords:\n",
    "    fns += glob(DATA_PATH + f\"cache/**/{keyword}/**/*filtered.npz\")\n",
    "np.random.shuffle(fns)\n",
    "\n",
    "# create sub-sets for training and test data\n",
    "num_train_samples = int(len(fns) * ratio_train * percentage)\n",
    "num_test_samples = int(len(fns) * (1 - ratio_train) * percentage)\n",
    "\n",
    "data = []\n",
    "targets = []\n",
    "for fn in fns:\n",
    "    # load filtered data\n",
    "    data.append(np.load(fn, allow_pickle=True))\n",
    "    \n",
    "    # create target label based on filename\n",
    "    label_id = np.array([\"cache/\" + k in fn for k in keywords], dtype=int)    \n",
    "    target = np.ones([len(data[-1]), num_labels]) * label_id\n",
    "    targets.append(target)\n",
    "\n",
    "train_data = data[:num_train_samples]\n",
    "train_targets = targets[:num_train_samples]\n",
    "\n",
    "test_data = data[num_train_samples:num_train_samples+num_test_samples]\n",
    "test_targets = targets[num_train_samples:num_train_samples+num_test_samples]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYPlLH2e-lc8"
   },
   "source": [
    "## **Linear regression**\n",
    "\n",
    "To get a baseline how easy this task is, we use simple linear regression.\n",
    "\n",
    "We first flatten the data to have one big chunk of data, then we use Ridge regression from scikit-learn for classification. \n",
    "\n",
    "Afterwards we visualize the first 10 seconds. You can try different values for the regularization term *alpha*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "EriHevOu-d6T",
    "outputId": "d3872743-0270-4c4a-8b13-ee76a9eee01e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# flatten data\n",
    "train_data_flat = np.reshape(train_data, [-1, num_channels])\n",
    "train_targets_flat = np.reshape(train_targets, [-1, num_labels])\n",
    "\n",
    "test_data_flat = np.reshape(test_data, [-1, num_channels])\n",
    "test_targets_flat = np.reshape(test_targets, [-1, num_labels])\n",
    "\n",
    "# train linear model\n",
    "clf = Ridge(alpha=0.01)\n",
    "clf.fit(train_data_flat, train_targets_flat)\n",
    "\n",
    "# predict train and test data\n",
    "train_pred = clf.predict(train_data_flat)\n",
    "test_pred = clf.predict(test_data_flat)\n",
    "\n",
    "# visualize the first seconds\n",
    "\n",
    "train_time = np.arange(0, len(train_data_flat) / preprocessed_sr, 1 / preprocessed_sr)\n",
    "test_time = np.arange(0, len(test_data_flat) / preprocessed_sr, 1 / preprocessed_sr)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax0.set_title(\"Train\")\n",
    "ax0.set_xlabel(\"time (s)\")\n",
    "ax0.set_ylabel(\"amplitude\")\n",
    "ax0.set_xlim([0, 5])\n",
    "ax0.set_ylim([-.5, 1.5])\n",
    "ax0.plot(train_time, train_pred, '--')\n",
    "plt.gca().set_prop_cycle(None)\n",
    "ax0.plot(train_time, train_targets_flat)\n",
    "ax0.legend(keywords)\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.set_title(\"Test\")\n",
    "ax1.set_xlabel(\"time (s)\")\n",
    "ax1.set_ylabel(\"amplitude\")\n",
    "ax1.set_xlim([0, 5])\n",
    "ax1.set_ylim([-0.5, 1.5])\n",
    "ax1.plot(test_time, test_pred, '--')\n",
    "plt.gca().set_prop_cycle(None)\n",
    "ax1.plot(test_time, test_targets_flat)\n",
    "ax1.legend(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xmdsTnQ-urc"
   },
   "source": [
    "### **Testing**\n",
    "\n",
    "In order to obtain an accuracy score for the linear model, we predict each sample individually and sum up the prediction. The most active output channel is the predicted target.\n",
    "\n",
    "For the keywords **yes** vs **no** and using the complete dataset, the accuracy should settle around 89%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "oXeUhwBf-o6D",
    "outputId": "9a0315c8-1f7b-4639-b733-6605b75127bc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for i, d in tqdm(enumerate(test_data)):\n",
    "    pred = clf.predict(d)\n",
    "    pred_label = np.argmax(np.sum(pred, axis=0))\n",
    "    true_label = np.argmax(np.sum(test_targets[i], axis=0))\n",
    "    \n",
    "    true_labels.append(true_label)\n",
    "    pred_labels.append(pred_label)    \n",
    "\n",
    "rr_acc = accuracy_score(true_labels, pred_labels)\n",
    "rr_cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f\"\\n\\ntest accuracy {rr_acc}\")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(rr_cm)\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks([0, 1], [\"predicted \" + k for k in keywords])\n",
    "plt.yticks([0, 1], keywords)\n",
    "for i, _ in enumerate(rr_cm):\n",
    "    for j, _ in enumerate(rr_cm[i]):\n",
    "        plt.text(j, i, rr_cm[i, j], fontsize=20, color='black')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQR0nnb7-0l1"
   },
   "source": [
    "# **Rockpool**\n",
    "\n",
    "## **Dataformat**\n",
    "\n",
    "Before getting into the spiking neural network models, let's bring the data into a format which is understood by Rockpool.\n",
    "\n",
    "In Rockpool there are two different kinds of Timeseries classes, TSContinuous and TSEvent. \n",
    "TSContinuous is used for time-continuous data such as the filtered audio samples in this task. The TSEvent class is used for spiking data which is used in spiking layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "zBON8ABi-x3r",
    "outputId": "ab512187-d8bb-4294-8a91-3ca6d1f31d2f"
   },
   "outputs": [],
   "source": [
    "from rockpool.timeseries import TSContinuous, TSEvent\n",
    "\n",
    "ts_train = TSContinuous(train_time, train_data_flat, name='Train data') \n",
    "ts_test = TSContinuous(test_time, test_data_flat, name='Test data') \n",
    "ts_train_targets = TSContinuous(train_time, train_targets_flat, name=\"Train targets\")\n",
    "ts_test_targets = TSContinuous(test_time, test_targets_flat, name=\"Test targets\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax0.set_xlim([0, 5])\n",
    "ts_train.plot(stagger=0.02)\n",
    "plt.gca().set_prop_cycle(None)\n",
    "ts_train_targets.plot()\n",
    "ax0.legend(keywords)\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.set_xlim([0, 5])\n",
    "ts_test.plot(stagger=0.02)\n",
    "plt.gca().set_prop_cycle(None)\n",
    "ts_test_targets.plot(linestyle='--')\n",
    "ax1.legend(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JcHMYDm_RK7"
   },
   "source": [
    "## **Reservoir computing**\n",
    "\n",
    "\n",
    "Now let's get into a spiking neural network model.\n",
    "In this case we want to train a reservoir computer (aka Liquid State Machine) to solve the task. We choose this model for several reasons:\n",
    "\n",
    "1) Dimensionality expansion. The data is expanded to a larger dimensionality which should make it easier to find a linear separatrix between the two different classes.\n",
    "\n",
    "2) Temporal integration. Using recurrent connections and the inherent temporal dynamics of Integrate-And-Fire neurons, the network has a certain amount of memory which allows to integrate the input over time.\n",
    "\n",
    "3) Low power. The activity in the reservoir can be very sparse and therefore lead to low power consumption.\n",
    "\n",
    "The LSM contains of three stages:\n",
    "\n",
    "First, we have to convert the time-continuous data to spikes. Here we use rate-coding, meaning the higher the power in a certain frequency, the more spikes are emitted.\n",
    "\n",
    "Second, we have to project the input spikes to a recurrently connected reservoir of IAF neurons. The most important parameters for the reservoir are the number of neurons, the synaptic and membrane time constants, the sparsity and distribution of weights.\n",
    "\n",
    "Third, the output layer has access to all reservoir neurons and uses the low-pass filtered (exponential) spike train to train a linear regressor (ridge regression).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=15PWoQV3bANlLh10eRkEfOb8-clEQ_vpw\" width=\"800\">\n",
    "\n",
    "\n",
    "Let's define these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ttL_mkL_0Xe"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "dt = 0.001 # sec\n",
    "\n",
    "# spike conversion\n",
    "weights_spike_conv = 3.0\n",
    "tau_mem_inp = 0.01 #sec\n",
    "\n",
    "# reservoir\n",
    "num_neurons = 100\n",
    "sparsity_inp = 0.5 # percent\n",
    "sparsity_res = 0.1 # percent\n",
    "bias = 0.0\n",
    "\n",
    "w_inp = np.random.normal(loc=0.0, scale=0.01, size=num_channels * num_neurons)\n",
    "w_inp[np.random.choice(range(len(w_inp)), size=int(len(w_inp) * (1 - sparsity_inp)))] = 0\n",
    "w_inp = w_inp.reshape(num_channels, num_neurons)\n",
    "\n",
    "w_rec = np.random.normal(loc=-0.0001, scale=0.0002, size=num_neurons * num_neurons)\n",
    "w_rec[np.random.choice(range(len(w_inp)), size=int(len(w_inp) * (1 - sparsity_res)))] = 0\n",
    "w_rec = w_rec.reshape(num_neurons, num_neurons)\n",
    "\n",
    "tau_mem = tau_syn = 0.05\n",
    "\n",
    "# output\n",
    "tau_syn_out = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djHvghzjB3_I"
   },
   "source": [
    "Now we can create the model using the parameters we just defined.\n",
    "\n",
    "\n",
    "For the spike conversion, we use a feed-forward Integrate-And-Fire (**FFIAF**) layer, for the reservoir a recurrent Integrate-And-Fire layer with spiking input (**RecIAFSpkIn**) and the readout layer is a feed-forward exponential filter (**FFExpSyn**).\n",
    "\n",
    "Here we use NEST as a backend the first two layers, so we add a **NEST** to the layer name but there are also other backends you could use such as Torch, Brian2 or JAX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifiUHXcMBztf"
   },
   "outputs": [],
   "source": [
    "from rockpool.layers import FFIAFNest, RecIAFSpkInNest, FFExpSyn\n",
    "from rockpool import Network\n",
    "\n",
    "# spike conversion\n",
    "lyr_inp = FFIAFNest(weights=np.eye(num_channels) * weights_spike_conv,\n",
    "                    tau_mem=tau_mem_inp,\n",
    "                    bias=0.,\n",
    "                    dt=dt,\n",
    "                    name='input')\n",
    "\n",
    "# reservoir\n",
    "lyr_res = RecIAFSpkInNest(weights_in=w_inp,\n",
    "                          weights_rec=w_rec,\n",
    "                          bias=bias,\n",
    "                          tau_mem=tau_mem,\n",
    "                          tau_syn_exc=tau_syn,\n",
    "                          tau_syn_inh=tau_syn,\n",
    "                          dt=dt,\n",
    "                          num_cores=1,\n",
    "                          name='reservoir')\n",
    "\n",
    "# linear readout\n",
    "lyr_out = FFExpSyn(weights=np.zeros([num_neurons, num_labels]),\n",
    "                   tau_syn=tau_syn_out,\n",
    "                   dt=dt,\n",
    "                   name=\"output\")\n",
    "\n",
    "net = Network(lyr_inp, lyr_res, lyr_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6M3ZI6bXCIBG"
   },
   "source": [
    "Now we can visualize the activity of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "Kc5kmI37B5Y-",
    "outputId": "1a5f8277-58a0-42c3-dc69-01d802560308"
   },
   "outputs": [],
   "source": [
    "net.reset_all()\n",
    "\n",
    "act = net.evolve(ts_train, duration=3., verbose=False)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(131)\n",
    "act['external'].plot(stagger=0.01)\n",
    "ax0.set_xlim([0, 3])\n",
    "ax0.set_ylim([0, 0.4])\n",
    "\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.set_title(\"Input\")\n",
    "act['input'].plot(s=1)\n",
    "\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.set_title(\"Reservoir\")\n",
    "act['reservoir'].plot(s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cy7LhNbXCaLR"
   },
   "source": [
    "## **Training**\n",
    "\n",
    "Let's train the model on each single training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "42knrzgMCJg9",
    "outputId": "00fce4dc-c7b2-40cf-c0c6-4b74c02388da"
   },
   "outputs": [],
   "source": [
    "time_single_sample = np.arange(0, len_samples / preprocessed_sr, 1 / preprocessed_sr)\n",
    "\n",
    "for i, d in tqdm(enumerate(train_data)):\n",
    "    net.reset_all()\n",
    "    \n",
    "    ts_inp = TSContinuous(time_single_sample, d)\n",
    "    ts_tgt = TSContinuous(time_single_sample, train_targets[i])\n",
    "\n",
    "    act = net.evolve(ts_inp, verbose=False)\n",
    "    \n",
    "    net.output.train_rr(ts_tgt,\n",
    "                        ts_input=act['reservoir'],\n",
    "                        regularize=0.001,\n",
    "                        train_biases=True,\n",
    "                        is_first=i == 0,\n",
    "                        is_last=i == len(train_data)-1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "WdwiO_MzCjVN",
    "outputId": "9aa6bce8-64d7-4358-bd90-91d94e95b90d"
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for i, d in tqdm(enumerate(test_data)):\n",
    "    net.reset_all()\n",
    "    \n",
    "    ts_inp = TSContinuous(time_single_sample, d)\n",
    "  \n",
    "    act = net.evolve(ts_inp, verbose=False)\n",
    "    pred = act['output'].samples\n",
    "    \n",
    "    pred_label = np.argmax(np.sum(pred, axis=0))\n",
    "    true_label = np.argmax(np.sum(test_targets[i], axis=0))\n",
    "    \n",
    "    pred_labels.append(pred_label)\n",
    "    true_labels.append(true_label)\n",
    "    \n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f\"\\n\\ntest accuracy {acc} {rr_acc}\")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.xticks([0, 1], [\"predicted \" + k for k in keywords])\n",
    "plt.yticks([0, 1], keywords)\n",
    "for i, _ in enumerate(cm):\n",
    "    for j, _ in enumerate(cm[i]):\n",
    "        plt.text(j, i, cm[i, j], fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3OYXU1TiDlR"
   },
   "source": [
    "## **Live demo**\n",
    "\n",
    "Before testing the model in a live setup, make sure to train it on a large portion of the data.\n",
    "\n",
    "You can use the following code to record some audio using your microphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIWIvcAYDc9t"
   },
   "outputs": [],
   "source": [
    "global microphone\n",
    "microphone = None\n",
    "\n",
    "def record(sec=1):\n",
    "    if not importlib.util.find_spec('google.colab') is None:\n",
    "\n",
    "\n",
    "        RECORD = \"\"\"\n",
    "        const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "        const b2text = blob => new Promise(resolve => {\n",
    "          const reader = new FileReader()\n",
    "          reader.onloadend = e => resolve(e.srcElement.result)\n",
    "          reader.readAsDataURL(blob)\n",
    "        })\n",
    "\n",
    "        var record = time => new Promise(async resolve => {\n",
    "          stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "          recorder = new MediaRecorder(stream)\n",
    "          chunks = []\n",
    "          recorder.ondataavailable = e => chunks.push(e.data)\n",
    "          recorder.start()\n",
    "          await sleep(time)\n",
    "          recorder.onstop = async ()=>{\n",
    "            blob = new Blob(chunks)\n",
    "            text = await b2text(blob)\n",
    "            resolve(text)\n",
    "          }\n",
    "          recorder.stop()\n",
    "        })\n",
    "        \"\"\"\n",
    "        \n",
    "        # run on colab\n",
    "        from google.colab import output\n",
    "        from base64 import b64decode\n",
    "        from io import BytesIO\n",
    "        if importlib.util.find_spec('pydub') is None:\n",
    "            !pip install pydub\n",
    "        if importlib.util.find_spec('pydub') is None:\n",
    "            sys.path.append(\"/usr/local/lib/python3.8/site-packages\")\n",
    "        from pydub import AudioSegment\n",
    "        \n",
    "        display(Javascript(RECORD))\n",
    "        s = output.eval_js('record(%d)' % (sec * 1000))\n",
    "        b = b64decode(s.split(',')[1])\n",
    "        audio = AudioSegment.from_file(BytesIO(b))\n",
    "        audio = np.array(audio.get_array_of_samples()) / audio.max_possible_amplitude\n",
    "        fr = len(audio) / sec\n",
    "        audio = librosa.resample(audio, fr, 16000)\n",
    "        return audio, 16000\n",
    "    \n",
    "    else:\n",
    "        # run local\n",
    "        import sounddevice as sd\n",
    "        import time\n",
    "        io_devices = sd.query_devices()\n",
    "        for (i, device) in enumerate(io_devices):\n",
    "            if \"default\" in device[\"name\"]:\n",
    "                samplerate = device['default_samplerate']\n",
    "                uid = i\n",
    "        \n",
    "        global audio_data, microphone\n",
    "        audio_data = np.array([])\n",
    "        def processdata(data, num_frames, t, status):\n",
    "            global audio_data\n",
    "            audio_data = np.append(audio_data, data)\n",
    "\n",
    "        if microphone is None:\n",
    "            microphone = sd.InputStream(device=uid, channels=1, samplerate=samplerate, callback=processdata)\n",
    "\n",
    "        microphone.start()\n",
    "        time.sleep(sec)\n",
    "        microphone.stop()\n",
    "        audio = librosa.resample(audio_data, samplerate, 16000)\n",
    "        return audio, 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlL3finEp0n4"
   },
   "source": [
    "When running the following box, you have *sec* seconds time to say one of the keywords into the microphone, which will be uploaded. Make sure to grant access rights to your microphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0z2AMMHJVlA1",
    "outputId": "498982b2-1699-4853-e1ae-3e78d5ada26d"
   },
   "outputs": [],
   "source": [
    "audio, fs = record(sec=2)\n",
    "print(len(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvPix1ReqEYF"
   },
   "source": [
    "Now, we got the raw audio data, but we have to filter it first. So, let's create a Butterworth filter layer using 32 filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dsDgQGnahW6"
   },
   "outputs": [],
   "source": [
    "from rockpool.layers import ButterMelFilter\n",
    "butter = ButterMelFilter(fs=fs, num_filters=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ25PZ-CqWp9"
   },
   "source": [
    "Ideally, we first normalize the raw audio before applying the Butterworth filter. Then we can use the previously trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "EXd9fWjua5QO",
    "outputId": "fcb68468-4e9f-4f6d-83e1-78d4242c5850"
   },
   "outputs": [],
   "source": [
    "butter.reset_all()\n",
    "net.reset_all()\n",
    "\n",
    "# normalize the raw audio\n",
    "audio /= np.max(np.abs(audio))\n",
    "\n",
    "# create a timeseries\n",
    "time_recording = np.arange(0, len(audio) / fs, 1 / fs)\n",
    "ts_inp = TSContinuous(time_recording, audio)\n",
    "\n",
    "# apply the Butterworth filter\n",
    "ts_filt = butter.evolve(ts_inp)\n",
    "\n",
    "# use the model for prediction\n",
    "act = net.evolve(ts_filt)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax0 = fig.add_subplot(151)\n",
    "ax0.set_title(\"Raw Audio\")\n",
    "ts_inp.plot()\n",
    "\n",
    "ax1 = fig.add_subplot(152)\n",
    "ax1.set_title(\"Filtered\")\n",
    "ts_filt.plot(stagger=0.01)\n",
    "\n",
    "ax2 = fig.add_subplot(153)\n",
    "ax2.set_title(\"Input\")\n",
    "act['input'].plot(s=1)\n",
    "\n",
    "ax3 = fig.add_subplot(154)\n",
    "ax3.set_title(\"Reservoir\")\n",
    "act['reservoir'].plot(s=1)\n",
    "\n",
    "ax4 = fig.add_subplot(155)\n",
    "ax4.set_title(\"Output\")\n",
    "act['output'].plot()\n",
    "ax4.legend(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9I68l11WrA99"
   },
   "source": [
    "Visually, we see that the model predicted the (in)correct keyword. Using the argmax of the summed activtiy of the readout layer should confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M_gjHSsEbhlv",
    "outputId": "93812071-419f-435e-d8c3-c41198764bb6"
   },
   "outputs": [],
   "source": [
    "keywords[np.argmax(np.sum(act['output'].samples, axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LxAFl579anP"
   },
   "source": [
    "## **Fine tuning**\n",
    "\n",
    "You can play around with the model to improve the performance further.\n",
    "Important parameters for the reseroir model are:\n",
    "\n",
    "\n",
    "\n",
    "*   Number of neurons\n",
    "*   Time constants (different combinations of synaptic and membrane time constants)\n",
    "*   Recurrent weights (strength, sparsity, structure)\n",
    "*   Input weights\n",
    "*   Different keywords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xe0Oq5swbmf3"
   },
   "outputs": [],
   "source": [
    "# Tune the model to improve performance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TwoWordClassifier_Complete",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
