{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Spiking Convolutional Neural Network for analysing DVS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and understanding the training data\n",
    "\n",
    "The data is stored in a folder of images, as often done while training neural networks. Each subfolder coincides with a class, and contains .png files of training examples.\n",
    "\n",
    "We use a standard Torchvision dataset and dataloaders to read the data into PyTorch. The transformation is needed to provide images with the correct scale and a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training frames: 192995\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "FOLDER = '/home/martino/Work/synoploss/mnist_dvs/data/images/train'\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def transform(image):\n",
    "    return ToTensor()(image)[0].unsqueeze(0) * 255\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    root=FOLDER,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "print(\"Number of training frames:\", len(train_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_dataset` object contains all our training images and labels, which are loaded into batches by the `train_dataloader` object.\n",
    "\n",
    "Using `train_dataset`, try looking at how the data looks like. Plot one of the samples, which are 64x64 images, and print the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to be left blank\n",
    "\n",
    "sample, label = train_dataset[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sample.squeeze())  # remove the extra dimension\n",
    "print('label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "We now define our convolutional neural network. It will be a small network with 3 convolutional layers and one fully connected. Note that so far we are doing the exact same thing that we would do with traditional deep networks. There are no spikes yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MNISTClassifier(torch.nn.Module):\n",
    "    def __init__(self, quantize=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=8,\n",
    "                            kernel_size=(3, 3), bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=12,\n",
    "                            kernel_size=(3, 3), bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            torch.nn.Conv2d(in_channels=12, out_channels=12,\n",
    "                            kernel_size=(3, 3), bias=False),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            torch.nn.Dropout2d(0.5),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(432, 10, bias=False),\n",
    "            torch.nn.ReLU(),   # note that it's needed, but odd, to add a ReLU at the end\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training phase\n",
    "\n",
    "We now want to train this network. Once again, this is no different from training a normal CNN. Only later, we will turn this network into a spiking network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model and transferring to GPU\n",
    "model = MNISTClassifier()\n",
    "model.cuda()\n",
    "\n",
    "# defining the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# defining the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19fca36b6944b198d70382602775c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=754.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up a training loop\n",
    "from tqdm.notebook import tqdm\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    for (images, labels) in progress_bar:\n",
    "        # move to the GPU\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        # reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass through the network\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # compute and backpropagate the loss\n",
    "        loss_value = criterion(outputs, labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix(LOSS=loss_value.item())\n",
    "        \n",
    "    # save the network, just in case\n",
    "    torch.save(model.state_dict(), 'digits_net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to a spiking network\n",
    "\n",
    "To convert to a spiking network, we use the `from_torch` tool from `sinabs`, which reads a network (must be sequential, and only certain layers are supported), and converts it to the `sinabs` Network object, which supports all the dynamics of neurons on top of the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the model from saved, if necessary\n",
    "model.load_state_dict(torch.load('digits_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_0 (8, 62, 62)\n",
      "avgpool_1 (8, 31, 31)\n",
      "conv2d_2 (12, 29, 29)\n",
      "avgpool_3 (12, 14, 14)\n",
      "conv2d_4 (12, 12, 12)\n",
      "avgpool_5 (12, 6, 6)\n",
      "flatten (432,)\n",
      "linear_7 (10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martino/Work/sinabs/sinabs/from_torch.py:354: UserWarning: Layer 'Dropout2d' is not supported. Skipping!\n",
      "  warn(f\"Layer '{type(module).__name__}' is not supported. Skipping!\")\n"
     ]
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "net = from_model(\n",
    "    model.seq,\n",
    "    input_shape=(1, 64, 64),\n",
    "    threshold=1.0,\n",
    "    membrane_subtract=1.0,\n",
    "    threshold_low=-5.0,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live demo\n",
    "\n",
    "Instead of properly testing our network on the test set, let's directly play with it in a real world scenario. We input DVS data into the network, and run it real-time, getting predictions about which digits were shown to the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martino/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/martino/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/martino/Work/aermanager/aermanager/liveaer.py\", line 47, in _build_q\n",
      "    frame = next(self.nfi).image.squeeze()\n",
      "  File \"/home/martino/.local/lib/python3.7/site-packages/dv/NetworkInput.py\", line 91, in __next__\n",
      "    super().__next__()\n",
      "  File \"/home/martino/.local/lib/python3.7/site-packages/dv/NetworkInput.py\", line 39, in __next__\n",
      "    self._receive_next_packet()\n",
      "  File \"/home/martino/.local/lib/python3.7/site-packages/dv/NetworkInput.py\", line 44, in _receive_next_packet\n",
      "    packet_data = self._socket.recv(length, socket.MSG_WAITALL)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from aermanager import LiveDv\n",
    "\n",
    "live = LiveDv(host='192.168.11.72', port=7777, qlen=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_0 (8, 62, 62)\n",
      "avgpool_1 (8, 31, 31)\n",
      "conv2d_2 (12, 29, 29)\n",
      "avgpool_3 (12, 14, 14)\n",
      "conv2d_4 (12, 12, 12)\n",
      "avgpool_5 (12, 6, 6)\n",
      "flatten (432,)\n",
      "linear_7 (10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martino/Work/sinabs/sinabs/from_torch.py:354: UserWarning: Layer 'Dropout2d' is not supported. Skipping!\n",
      "  warn(f\"Layer '{type(module).__name__}' is not supported. Skipping!\")\n"
     ]
    }
   ],
   "source": [
    "# we resize and crop our input so that it matches the training data\n",
    "adaptivepool = torch.nn.AdaptiveAvgPool2d((64, 64))\n",
    "resize_factor = 16\n",
    "\n",
    "def transform(x):\n",
    "    x = x[:, :, 2:-2, 45:-45]  # crop\n",
    "    x = torch.tensor(x).float().cuda()\n",
    "    x = adaptivepool(x) * resize_factor\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-de7d9c539d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/aermanager/aermanager/liveaer.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "while True:\n",
    "    batch = live.get_batch()\n",
    "    batch = transform(batch)\n",
    "\n",
    "    out = net(batch)\n",
    "    maxval, pred_label = torch.max(out.sum(0), dim=0)\n",
    "\n",
    "    THR = 30\n",
    "    clear_output()\n",
    "    if maxval > THR:\n",
    "        display(pred_label.item())\n",
    "    else:\n",
    "        display('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
