{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GszGjuVEz4lW"
   },
   "source": [
    "# Training a Spiking Convolutional Neural Network for analysing DVS data\n",
    "\n",
    "In this tutorial, we will train a **spiking convolutional neural network** in the most direct way: by training a normal CNN first, and then transferring the parameters we learned onto the spiking network.\n",
    "\n",
    "We train the network on the [MNIST-DVS](http://www2.imse-cnm.csic.es/caviar/MNISTDVS.html) dataset, which consists of MNIST digits (handwritten digits from 0 to 9) recorded with a DVS (dynamic vision sensor). DVSs are event-based sensors, whose data are well suited for elaboration in spiking networks (which are also based on events, the spikes). This is how the dataset looks like:\n",
    "\n",
    "![MNIST-DVS digit](https://drive.google.com/uc?id=1BsFc8x54_drHy7uB4qqhnVkuMXS8u58C)\n",
    "\n",
    "but remember that it's a stream of individual events, not frames! The data looks like this:\n",
    "```\n",
    "# x    y    time         polarity\n",
    "  203  129  1564882943   0\n",
    "  21   212  1564882951   1\n",
    "  ...\n",
    "```\n",
    "\n",
    "In order to train the CNN, we accumulate these events into frames, which are static pictures. I already did that for you, and the dataset will be downloaded below.\n",
    "\n",
    "## Installs and downloads\n",
    "\n",
    "Download some data we will use later. Some of this data is an elaboration of the MNIST_DVS dataset from the Seville Microelectronics Institute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "1s4F8fN80Fp8",
    "outputId": "c8c73da0-cf5f-419b-926f-7964c100b1d2"
   },
   "outputs": [],
   "source": [
    "! wget -O mnist_dvs_train_frames.zip https://www.dropbox.com/s/dl/lni9tyspaykxq0d/train.zip\n",
    "! unzip -q mnist_dvs_train_frames.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "B0rVMiVQpfYz",
    "outputId": "07501a2c-5db4-43db-94db-a18bfdfdf812"
   },
   "outputs": [],
   "source": [
    "! wget https://www.dropbox.com/s/dl/iebet9wbow38tn2/digits-videos.zip\n",
    "! unzip digits-videos.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MZcgTTDXAuE"
   },
   "source": [
    "Install `sinabs`, our library for Spiking Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "Hz-8i66oWvbP",
    "outputId": "865c743c-c1ef-4ba2-f0f5-4385da805264"
   },
   "outputs": [],
   "source": [
    "% pip install sinabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gkdl6zB7BdVT"
   },
   "source": [
    "## Loading and understanding spiking data\n",
    "\n",
    "Let's start by using the original DVS data, made of **events**, not frames. The file `number_dvs_recording.npz` contains a DVS stream that I recorded myself, standing in front of a DVS with numbers written on paper. It's not part of the MNIST-DVS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o-nOwmhAz4mO",
    "outputId": "30b39f60-d7bb-4dfe-900c-d5c7d7e9c933"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262628"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loading a DVS recording\n",
    "File = np.load('digits-A.npz')\n",
    "\n",
    "t, x, y = File['t'], File['x'], File['y']\n",
    "\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUfZyXOzDhxT"
   },
   "source": [
    "t, x, y now contain the times and locations of DVS events. Find a way to visualize this data in order to understand it a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "CernVmKyyDRV",
    "outputId": "ed2a00cf-3a3d-4c56-df1a-9035bf41c7bb"
   },
   "outputs": [],
   "source": [
    "# exercise\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zq7RCcqKz4lZ"
   },
   "source": [
    "Now we'll **put this event-based data aside** and use accumulated data, in order to train a normal CNN. I've turned this event-based data into a frame-based video, and we'll use these frames for training.\n",
    "\n",
    "## Loading and understanding the training data\n",
    "\n",
    "The data is stored in a folder of images, as often done while training neural networks. Each subfolder coincides with a class, and contains .png files of training examples. These are **frames**, not events. We are training a normal CNN, for now.\n",
    "\n",
    "We use a standard Torchvision dataset and dataloaders to read the data into PyTorch. The transformation is needed to provide images with the correct scale and a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gPUTCc5Wz4lc",
    "outputId": "e45e1fdc-c238-40fc-933c-46d886ed73d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training frames: 192995\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor, RandomAffine\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "FOLDER = './train'\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "rescaler = RandomAffine(0, scale=(0.6, 1.0), translate=(0.2, 0.2))\n",
    "\n",
    "def transform(image):\n",
    "    image = rescaler(image)\n",
    "    return ToTensor()(image)[0].unsqueeze(0) * 255\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    root=FOLDER,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "print(\"Number of training frames:\", len(train_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xSo19gXz4lk"
   },
   "source": [
    "The `train_dataset` object contains all our training images and labels, which are loaded into batches by the `train_dataloader` object.\n",
    "\n",
    "Using `train_dataset`, try looking at how the data looks like. Plot one of the samples, which are 64x64 images, and print the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "_KZi9R66z4ll",
    "outputId": "049218e1-9d32-4b31-8f4d-a3b3b97847c4"
   },
   "outputs": [],
   "source": [
    "# exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFxZoZLGz4lp"
   },
   "source": [
    "## Defining a model\n",
    "\n",
    "We now define our convolutional neural network. It will be a small network with 3 convolutional layers and one fully connected. Note that so far we are doing the exact same thing that we would do with traditional deep networks. There are no spikes yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6x4tOVhz4ls"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq = nn.Sequential(*[\n",
    "            nn.Conv2d(in_channels=1, out_channels=8,\n",
    "                      kernel_size=(3, 3), bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(in_channels=8, out_channels=32,\n",
    "                      kernel_size=(3, 3), bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16,\n",
    "                      kernel_size=(3, 3), bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(576, 32, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10, bias=False),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBirWRIcz4lv"
   },
   "source": [
    "## Main training phase\n",
    "\n",
    "We now want to train this network. Once again, this is no different from training a normal CNN -- it *is* a normal CNN. Only later, we will turn this network into a spiking network.\n",
    "\n",
    "The next two cells implement the following, in PyTorch:\n",
    " - instantiate the model and copy it to the GPU\n",
    " - instantiate the loss (cross entropy?)\n",
    " - instantiate an optimizer\n",
    " - write a training loop\n",
    " - train for an epoch or two, checking that the loss improves\n",
    " - test (on the training set, for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_44MFUuKz4lx"
   },
   "outputs": [],
   "source": [
    "# instantiating the model and transferring to GPU\n",
    "model = MNISTClassifier()\n",
    "model.cuda()\n",
    "\n",
    "# defining the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# defining the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "a88cd13822be401ba67709f949b9e970",
      "168a4a08d9d94af8917791a7b9c54c7b",
      "16a2bbeff18f4450965ac56011cd2c7f",
      "c80225212d644840b630949d9762770c",
      "3cadcec7929c4f22a0fe9067a1c00956",
      "eb248273dd4a4bf685f85eb808262d59",
      "f4d07d5e09a1434fb39bc0b0b65ad660",
      "0c53f5564aff4e92aaf59f0f4041bbcb"
     ]
    },
    "colab_type": "code",
    "id": "gOjky1YFz4l0",
    "outputId": "a4751d00-88c0-4d7a-927e-baef52ff52a3"
   },
   "outputs": [],
   "source": [
    "# Set up a training loop\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "n_epochs = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    for (images, labels) in progress_bar:\n",
    "        # move to the GPU\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        # reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass through the network\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # compute and backpropagate the loss\n",
    "        loss_value = criterion(outputs, labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix(LOSS=loss_value.item())\n",
    "\n",
    "    # quickly test on one batch of the training set\n",
    "    _, predictions = torch.max(outputs, axis=1)\n",
    "    fraction_correct = (predictions == labels).sum().item() / BATCH_SIZE\n",
    "    print(\"Accuracy on training set:\", fraction_correct)\n",
    "        \n",
    "    # save the network, just in case\n",
    "    torch.save(model.state_dict(), 'digits_net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xkJYLCcz4l3"
   },
   "source": [
    "## Converting to a spiking network\n",
    "\n",
    "To convert to a spiking network, we use the `from_torch` tool from `sinabs`, which reads a network (must be sequential, and only certain layers are supported), and converts it to the `sinabs` Network object, which supports all the dynamics of neurons on top of the convolutions.\n",
    "\n",
    "First, we reload our weights, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pIMcMET8z4l4",
    "outputId": "1b125dbd-0061-4cd7-b4e3-b0f27d154e18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the model from saved, if necessary\n",
    "model.load_state_dict(torch.load('digits_net.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ayTb45kTth2"
   },
   "source": [
    "Turn the model into a spiking network with sinabs's tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "UzijVuqJz4l9",
    "outputId": "bddca52e-e7d4-4811-f355-9b4c434356ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_0 (8, 62, 62)\n",
      "avgpool_1 (8, 31, 31)\n",
      "conv2d_2 (32, 29, 29)\n",
      "avgpool_3 (32, 14, 14)\n",
      "conv2d_4 (16, 12, 12)\n",
      "avgpool_5 (16, 6, 6)\n",
      "flatten (576,)\n",
      "linear_7 (32,)\n",
      "linear_8 (10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martino/Work/sinabs/sinabs/from_torch.py:354: UserWarning: Layer 'Dropout2d' is not supported. Skipping!\n",
      "  warn(f\"Layer '{type(module).__name__}' is not supported. Skipping!\")\n"
     ]
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "net = from_model(\n",
    "    model.seq,\n",
    "    input_shape=(1, 64, 64),\n",
    "    threshold=1.0,\n",
    "    membrane_subtract=1.0,\n",
    "    threshold_low=-5.0,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unFN2iXig_ch"
   },
   "source": [
    "## Preparing the data for testing\n",
    "\n",
    "When using our neuromorphic chips, we will feed the DVS events to the network one by one, live, as soon as they are received. However, here, we are only simulating the chip, and it's necessary to have a finite time step.\n",
    "\n",
    "To simulate the very high frame rate, we feed 10 milliseconds long frames to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4PG_DIRrSUw0",
    "outputId": "649759a8-78fd-4464-d71e-3a8d8b373e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([521, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# time bin size\n",
    "TIMESTEP_LENGTH = 10  # milliseconds\n",
    "\n",
    "binned_input = np.histogramdd((t, x, y), bins=(np.arange(t.min(), t.max(), 1000 * TIMESTEP_LENGTH), 64, 64))[0]\n",
    "binned_input_tensor = torch.tensor(binned_input).float().unsqueeze(1).cuda()\n",
    "\n",
    "print(binned_input_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bv3ZdgPh8OG"
   },
   "source": [
    "The dimensions of `binned_input_tensor` correspond to (time, channels, height, width).\n",
    "\n",
    "As an exercise, pass these time steps (or some of them) into the network (calling `net(...)`) and read the output. The output will be a tensor of dimensions (time, output_neuron_number). Each output neuron corresponds to a digit from 0 to 9, the predicted number recorded in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkNuCqfHS2ic"
   },
   "outputs": [],
   "source": [
    "#exercise\n",
    "output = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0cAcVzmjCiv"
   },
   "source": [
    "\n",
    "Now, find a good way to see what's the network's prediction and whether it changes in time. The **maximally active neuron** corresponds to the network's prediction at a given time.\n",
    "\n",
    "- Try finding out which digit was shown to the sensor\n",
    "- Try loading the other files `digits-B.npz`, `digits-C.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "yOXlZPLUS3w7",
    "outputId": "f7b10840-20be-4799-dfe0-ab486f1f1d44"
   },
   "outputs": [],
   "source": [
    "# exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLPGU4Kvuyia"
   },
   "source": [
    "You should see one neuron being particularly active. Does it correspond to the digit shown to the DVS in the video?\n",
    "\n",
    "**You can now go back and experiment with the other two videos, `digits-B.npz` and `digits-C.npz`.**\n",
    "\n",
    "## Estimating power consumption\n",
    "\n",
    "Sinabs can count the number of synaptic operations performed in the last forward pass. We estimate an energy consumption of 10 pJ per synaptic operation for our chips; multiplied by the number of SynOps per second, we get the power consumption during the video analysis.\n",
    "\n",
    "Note that if nothing was happening in the video, there would be close to no energy use at all.\n",
    "\n",
    "Also consider that we did nothing to encourage the network to keep the number of SynOps low! Better results can be achieved by optimising in this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TaPZgnBNSxUa"
   },
   "outputs": [],
   "source": [
    "# useful constants that describe the power consumption of our chip\n",
    "SYNOP_POWER = 10e-8  # millijoules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Events_routed</th>\n",
       "      <th>Fanout_Prev</th>\n",
       "      <th>In</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Out</th>\n",
       "      <th>SynOps</th>\n",
       "      <th>SynOps/s</th>\n",
       "      <th>Time_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conv2d</td>\n",
       "      <td>791398.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>791398.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>791398.0</td>\n",
       "      <td>pooling2d</td>\n",
       "      <td>791398.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>227922624.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>791398.0</td>\n",
       "      <td>conv2d</td>\n",
       "      <td>749759.0</td>\n",
       "      <td>227922624.0</td>\n",
       "      <td>4.374714e+08</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>749759.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>749759.0</td>\n",
       "      <td>pooling2d</td>\n",
       "      <td>749759.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107965296.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>749759.0</td>\n",
       "      <td>conv2d</td>\n",
       "      <td>234082.0</td>\n",
       "      <td>107965296.0</td>\n",
       "      <td>2.072271e+08</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>234082.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>234082.0</td>\n",
       "      <td>pooling2d</td>\n",
       "      <td>234082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>234082.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>234082.0</td>\n",
       "      <td>flatten</td>\n",
       "      <td>234082.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7490624.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>234082.0</td>\n",
       "      <td>conv1d</td>\n",
       "      <td>14567.0</td>\n",
       "      <td>7490624.0</td>\n",
       "      <td>1.437740e+07</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>145670.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14567.0</td>\n",
       "      <td>conv1d</td>\n",
       "      <td>652.0</td>\n",
       "      <td>145670.0</td>\n",
       "      <td>2.795969e+05</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Events_routed  Fanout_Prev        In      Layer       Out       SynOps  \\\n",
       "0            0.0         72.0       0.0     conv2d  791398.0          0.0   \n",
       "1       791398.0          1.0  791398.0  pooling2d  791398.0          0.0   \n",
       "2    227922624.0        288.0  791398.0     conv2d  749759.0  227922624.0   \n",
       "3       749759.0          1.0  749759.0  pooling2d  749759.0          0.0   \n",
       "4    107965296.0        144.0  749759.0     conv2d  234082.0  107965296.0   \n",
       "5       234082.0          1.0  234082.0  pooling2d  234082.0          0.0   \n",
       "6       234082.0          1.0  234082.0    flatten  234082.0          0.0   \n",
       "7      7490624.0         32.0  234082.0     conv1d   14567.0    7490624.0   \n",
       "8       145670.0         10.0   14567.0     conv1d     652.0     145670.0   \n",
       "\n",
       "       SynOps/s  Time_window  \n",
       "0  0.000000e+00        521.0  \n",
       "1  0.000000e+00        521.0  \n",
       "2  4.374714e+08        521.0  \n",
       "3  0.000000e+00        521.0  \n",
       "4  2.072271e+08        521.0  \n",
       "5  0.000000e+00        521.0  \n",
       "6  0.000000e+00        521.0  \n",
       "7  1.437740e+07        521.0  \n",
       "8  2.795969e+05        521.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints a summary of operations in each layer\n",
    "net.get_synops(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Xrrof5mnGl4",
    "outputId": "2ed01f06-3dd5-4444-c649-d476df68deee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean power consumption (mW): 6.593554971209212\n"
     ]
    }
   ],
   "source": [
    "total_synops = net.get_synops(0)['SynOps'].sum()\n",
    "synops_per_millisecond = total_synops / len(binned_input) / TIMESTEP_LENGTH\n",
    "synops_per_second = synops_per_millisecond * 1000\n",
    "power_consumption_mW = synops_per_second * SYNOP_POWER\n",
    "\n",
    "print('Mean power consumption (mW):', power_consumption_mW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving\n",
    "\n",
    "- Better training procedures can be implemented specifically for spiking networks, but they are far slower, involving backpropagation through time. For certain tasks, and when optimizing for energy, these are a better option.\n",
    "- Power consumption has not been optimized at all\n",
    "- Other tasks, such as the analysis of audio or longer videos, require better techniques than frame-based training. In the afternoon we will see demos that use recurrent networks for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frI_5I3FuY93"
   },
   "source": [
    "## Final tasks\n",
    "\n",
    "Try improving the energy consumption by reducing the number of spikes. To do this, you can **manually rescale the weights of the first convolutional layer** after training. See how the energy consumption changes, and how the accuracy decreases, if it does. You can also try changing the network structure as you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_dvs_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c53f5564aff4e92aaf59f0f4041bbcb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "168a4a08d9d94af8917791a7b9c54c7b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16a2bbeff18f4450965ac56011cd2c7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb248273dd4a4bf685f85eb808262d59",
      "max": 754,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cadcec7929c4f22a0fe9067a1c00956",
      "value": 754
     }
    },
    "3cadcec7929c4f22a0fe9067a1c00956": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a88cd13822be401ba67709f949b9e970": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16a2bbeff18f4450965ac56011cd2c7f",
       "IPY_MODEL_c80225212d644840b630949d9762770c"
      ],
      "layout": "IPY_MODEL_168a4a08d9d94af8917791a7b9c54c7b"
     }
    },
    "c80225212d644840b630949d9762770c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c53f5564aff4e92aaf59f0f4041bbcb",
      "placeholder": "​",
      "style": "IPY_MODEL_f4d07d5e09a1434fb39bc0b0b65ad660",
      "value": "100% 754/754 [01:36&lt;00:00,  8.21it/s, LOSS=0.427]"
     }
    },
    "eb248273dd4a4bf685f85eb808262d59": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4d07d5e09a1434fb39bc0b0b65ad660": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
