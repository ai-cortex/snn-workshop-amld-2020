{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"report-header\"><div class=\"aictx-logo\"></div>\n",
    "<span class=\"report-type\">Demonstration</span><br />\n",
    "<span class=\"report-author\">Author: Felix Bauer</span><br />\n",
    "<span class=\"report-date\">25th January, 2020</span>\n",
    "</div><h1>Live Demo:</h1><h1>ECG anomaly detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how recurrent SNNs can be used for anomaly detection in an electrocardiogram (ECG) signal. The main part of the network will run on a DYNAP-SE neuromorphic processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "### --- Constants\n",
    "DT_ECG = 0.002778\n",
    "NUM_ECG_LEADS = 2\n",
    "NUM_ANOM_CLASSES = 5\n",
    "MAX_FANIN = 64  # Max. number of presynaptic connections per neuron (hardware limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Our goal is to detect five different classes of anomalies in a two-lead ECG signal from the MIT-BIH Arrithmia Database [ref]. An SNN that fulfills this task can, for instance, be used in a wearable ECG monitoring device to trigger an alarm in presence of pathological patterns. Below we see examples for a normal ECG signal and for each anomaly type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from scripts.plot_example_beats import plot_examples, labels\n",
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware\n",
    "\n",
    "We will first run a software simulation of our SNN and then run the network directly on a DYNAP-SE neuromorphic processor. The device we are demonstrating here is a prototype that imposes a few restrictions on the network, which are described below and which will also be considered in the software simulations.\n",
    "\n",
    "## Core-wise parameters\n",
    "Neuron and synapse parameters, such as time constants, firing thresholds and weights are set per core. The present processor consists of 16 cores of 256 neurons each. \n",
    "\n",
    "## Discrete weights\n",
    "Synaptic weights are the same for each postsynaptic neuron on a core, resulting in ternary weights: positive (excitatory), negative (inhibitory), and zero (not connected). However, between each pair of neurons, multiple connections are possible, which effectively allows for integer weights.\n",
    "\n",
    "## Connectivity\n",
    "The number of presynaptic connections to each neuron is generally limited to 64.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "To load the ECG data we will use a data loader class that is specifically written for this purpose. You find its source code in the folder of this tutorial. \n",
    "The ECG data itself can be found at http://physionet.org/physiobank/database/mitdb/. We extracted the signal and its annotations to a .npy-file and a .csv file that can be found in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG signal and annotaitions have been loaded from /home/felix/gitlab/Projects/AnomalyDetection/ECG/ecg_recordings\n"
     ]
    }
   ],
   "source": [
    "from scripts.dataloader import ECGDataLoader\n",
    "\n",
    "# - Object to load ECG data\n",
    "data_loader = ECGDataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal-to-spike encoding\n",
    "\n",
    "The analog ECG signal is converted to trains of events through a sigma-delta encoding scheme. For every ECG lead there are two output channels, emitting spikes when the input signal increases (\"up\"-channel) or decreases (\"down\"-channel) by a specified amount.\n",
    "\n",
    "The four resulting spike trains serve as input to the acutal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import FFUpDown\n",
    "\n",
    "# - Spike encoding\n",
    "spike_enc = FFUpDown(\n",
    "    weights=NUM_ECG_LEADS,\n",
    "    dt=DT_ECG,    \n",
    "    thr_up=0.1,\n",
    "    thr_down=0.1,\n",
    "    multiplex_spikes=True,\n",
    "    name=\"spike_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture\n",
    "\n",
    "\n",
    "To detect the anomalies we will use a reservoir network consisting of three layers:\n",
    "\n",
    "## Input expansion layer\n",
    "\n",
    "This layer consists of 128 neurons, each of which with up to 64 presynaptic excitatory connections (or a positive integer weight up to 64) to one of the four input channels. This connection scheme ensures that the neurons respond differently to the input, therefore increasing the dimensionality of the signal. \n",
    "\n",
    "This is enhanced by the fact that the hardware neurons on the DYNAP-SE slightly vary in their individual characteristics, which will result in richer neuron dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Weights of dimensionality expansion layer')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# - Input expansion layer\n",
    "num_ch_in = 2 * NUM_ECG_LEADS\n",
    "size_expand = 128\n",
    "baseweight_expand = 5e-4\n",
    "\n",
    "# weights_expand = np.zeros((num_ch_in, size_expand))\n",
    "# num_input_conns = np.random.randint(1, MAX_FANIN + 1, size=size_expand)\n",
    "# num_neur_per_ext = int(np.floor(size_expand / num_ch_in))\n",
    "# for idx_ch_in in range(num_ch_in):\n",
    "#     idcs_inp = slice(idx_ch_in * num_neur_per_ext, (idx_ch_in + 1) * num_neur_per_ext)\n",
    "#     weights_expand[idx_ch_in, idcs_inp] = num_input_conns[idcs_inp]\n",
    "#\n",
    "# np.save(\"weights/weights_expand.npy\", weights_expand)\n",
    "    \n",
    "weights_expand = np.load(\"network/weights_expand.npy\")\n",
    "\n",
    "plt.imshow(weights_expand, aspect=\"auto\")\n",
    "plt.title(\"Weights of dimensionality expansion layer\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reservoir layer\n",
    "\n",
    "The reservoir layer consists of 512 excitatory and neuron 128 inhibitory neurons which are connected recurrently in a stochastic manner. Due to the recurrent connections, the network state at a given time does not only depend on the current input but also on previous network states, therefore implicitly encoding information about past inputs. This makes it possible to processs temporal relations in the input signal.\n",
    "\n",
    "As a result, the state of each neuron in the reservoir is a function of the history of the input signal. If these functions are sufficiently independent from each other, they can be combined to approximate arbitrary functions. This linear combniation is done by the readout layer. In other words, the reservoir projects the signal into a high-dimensional state space, which ideally allows a linear separation of the different classes.\n",
    "\n",
    "The partition into excitatory and inhibitory neurons is not strictly required but makes it easier to control neuron dynamics on the neuromorhpic processor.\n",
    "\n",
    "In the following we will use a function provided by rockpool to generate the reservoir weights. For the excitatory recurrent connections the neurons are assumed to lie on a 2D grid and two neurons are more likely to be connected the closer they are to each other. Apart from that there are long-range connections which are independent of the neurons' positions on the grid.\n",
    "\n",
    "Finally, we will assume the input expansion layer and the reservoir to be one layer. This is a technicality that makes possible the readout to access the neuron activities in the former layer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.weights import partitioned_2d_reservoir\n",
    "\n",
    "## -- Reservoir layer\n",
    "size_rec = 512\n",
    "size_inh = 128\n",
    "size_reservoir = size_rec + size_inh\n",
    "\n",
    "# - Connections to excitatory layer\n",
    "num_exp_rec = 16\n",
    "num_inh = 16\n",
    "# - Connections to inhibitory layer\n",
    "num_rec_inh = 64\n",
    "# - Recurrent connections\n",
    "num_rec_short = 24  # Connections to neighbors\n",
    "num_rec_long = 8  # Long-range connections\n",
    "\n",
    "baseweight_exp_rec = 8e-5\n",
    "baseweight_rec = 8e-5  # 1.75e-4\n",
    "baseweight_rec_inh = 8e-5\n",
    "baseweight_inh = 1e-4\n",
    "\n",
    "# Fill expansion weights with 0s to fit full reservoir size\n",
    "weights_res_in = np.hstack((weights_expand, np.zeros((num_ch_in, size_reservoir))))\n",
    "\n",
    "# weights_rec = partitioned_2d_reservoir(\n",
    "#     size_in=size_expand,\n",
    "#     size_rec=size_rec,\n",
    "#     size_inhib=size_inh,\n",
    "#     max_fanin=MAX_FANIN,\n",
    "#     num_inp_to_rec=num_exp_rec,\n",
    "#     num_rec_to_inhib=num_rec_inh,\n",
    "#     num_inhib_to_rec=num_inh,\n",
    "#     num_rec_short=num_rec_short,\n",
    "#     num_rec_long=num_rec_long,\n",
    "#     width_neighbour=(2.0, 2.0),\n",
    "# )\n",
    "# np.save(\"weights/weights_rec.npy\", weights_rec)\n",
    "\n",
    "weights_rec = np.load(\"network/weights_rec.npy\")\n",
    "\n",
    "# Scale weights for software simulation\n",
    "weights_res_in_scaled = weights_res_in.copy() * baseweight_expand\n",
    "\n",
    "start_rec = size_expand\n",
    "start_inh = size_expand + size_rec\n",
    "\n",
    "weights_rec_scaled = weights_rec.copy()\n",
    "\n",
    "weights_rec_scaled[:start_rec, start_rec: start_inh] *= baseweight_exp_rec\n",
    "weights_rec_scaled[start_rec: start_inh, start_rec: start_inh] *= baseweight_rec\n",
    "weights_rec_scaled[start_rec: start_inh, start_inh:] *= baseweight_rec_inh\n",
    "weights_rec_scaled[start_inh:, start_rec: start_inh] *= baseweight_inh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Presynaptic neurons')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.imshow(weights_res_in_scaled, aspect=\"auto\")\n",
    "plt.title(\"Connections from expansion layer to reservoir\")\n",
    "plt.xlabel(\"Postsynaptic neurons\")\n",
    "plt.ylabel(\"Presynaptic neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f76c27baf74493988390c8e4a23b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Presynaptic neurons')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.imshow(weights_rec_scaled, aspect=\"auto\")\n",
    "plt.title(\"Recurrent reservoir connections\")\n",
    "plt.xlabel(\"Postsynaptic neurons\")\n",
    "plt.ylabel(\"Presynaptic neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import RecIAFSpkInNest\n",
    "\n",
    "# - Load reservoir parameters from file (generated with gen_params.py)\n",
    "kwargs_reservoir = dict(np.load(\"network/kwargs_reservoir.npz\"))\n",
    "\n",
    "# - Instantiate reservoir layer object\n",
    "reservoir = RecIAFSpkInNest(\n",
    "    weights_in=weights_res_in,\n",
    "    weights_rec=weights_rec,\n",
    "    name=\"reservoir\",\n",
    "    **kwargs_reservoir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout layer\n",
    "\n",
    "The readout layer low-pass filters the reservoir spike trains to obtain an analog signal. It is then trained by ridge regression to perform a linear separation between the ECG anomaly types. There is one readout unit for each anomaly and the corresponding target is 1 whenever the anomaly is present and 0 otherwise.\n",
    "\n",
    "Although the linear regression algorithm is traditionally not for classification tasks, we use it here because it is very fast and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import FFExpSyn\n",
    "\n",
    "readout = FFExpSyn(\n",
    "    weights=np.zeros((size_full, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool import Network\n",
    "\n",
    "# - Network that holds the layers\n",
    "sw_net = Network(spike_enc, reservoir, readout, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In the following we will train the (software) readout layer. For this we generate batches of ECG data and evolve the network with it as input. After each batch the readout weights are updated.\n",
    "\n",
    "We have trained the readout beforehand and will simply load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # - Generator that yields batches of ECG data\n",
    "# batchsize_training = 1000\n",
    "# num_beats = 10000\n",
    "# regularize = 0.1\n",
    "# batch_gen = data_loader.get_batch_generator(num_beats=num_beats, batchsize=batchsize_training)\n",
    "\n",
    "# t_start = time.time()\n",
    "# for batch in batch_gen:\n",
    "#     output = sw_net.evolve(batch.input)\n",
    "#     readout.train_rr(\n",
    "#         batch.target,\n",
    "#         output[\"reservoir\"],\n",
    "#         is_first=batch.is_first,\n",
    "#         is_last=batch.is_last,\n",
    "#         regularize=regularize,\n",
    "#     )\n",
    "    \n",
    "# sw_net.reset_all()\n",
    "# print(f\"Trained network in {time.time() - t_start:.2f} seconds.\")\n",
    "\n",
    "# np.save(\"network/readout_weights\", readout.weights)\n",
    "# np.save(\"network/readout_bias\", readout.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Test on training data\n",
    "# target = batch.target.start_at_zero()\n",
    "# res_data = output[\"reservoir\"]\n",
    "\n",
    "# test_on_training = readout.evolve(res_data.start_at_zero())\n",
    "# readout.reset_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# from rockpool import TSContinuous\n",
    "# test_on_training.clip(channels=4).plot()\n",
    "# target.clip(channels=4).plot()\n",
    "# any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "# any_target.plot(color=\"gray\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-131c5d63dbd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network/readout_weights.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network/readout_bias.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readout' is not defined"
     ]
    }
   ],
   "source": [
    "readout.weights = np.load(\"network/readout_weights.npy\")\n",
    "readout.bias = np.load(\"network/readout_bias.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can now test our network to see how it performs with data it has not been trained on.\n",
    "The plots below show the output of each readout unit (blue). The targets are plotted in orange. \n",
    "\n",
    "Because the readout units are only trained to distinguish between normal and one specific anomaly, there many cross-detections. We therefore also plot a gray curve that indicates the presence of _any_ anomaly. For many applications it is sufficient to know that there is an anomaly. If one needs to classify which type it is, one could, for instnace, train an all-vs-all classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sw_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-df364cb8901e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mecg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_beats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msw_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"readout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msw_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sw_net' is not defined"
     ]
    }
   ],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "num_beats = 100\n",
    "ecg_data = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "net_data = sw_net.evolve(ecg_data.input)\n",
    "output = net_data[\"readout\"]\n",
    "sw_net.reset_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0bd1f56d664ef0ab15b13fe98d425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "target = ecg_data.target\n",
    "any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(8, 10))\n",
    "axes[-1, -1].set_visible(False)\n",
    "plt.subplots_adjust(\n",
    "    top=0.98, bottom=0.05, left=0.15, right=0.95, hspace=0.5, wspace=0.2\n",
    ")\n",
    "\n",
    "for i_anom, (ax, lbl) in enumerate(zip(axes.flatten()[:-1], labels)):\n",
    "    output.clip(channels=i_anom).plot(target=ax)\n",
    "    any_target.plot(target=ax, color=\"gray\", alpha=0.5)\n",
    "    target.clip(channels=i_anom).plot(target=ax)\n",
    "    ax.set_title(lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Implementation\n",
    "\n",
    "Now it is time to replace the software reservoir with the neuromorphic processor. As weights, we can simply use the (unscaled) integer weights from which we also generated the weights of the software reservoir.\n",
    "\n",
    "Neuron and synapse parameters will be loaded from a file directly onto the chip. They are chosen so that the resulting dynamics are close to that of the simulation.\n",
    "\n",
    "Note that the hardware parameters are often refered to as \"biases\", because they correspond to biases in circuits on the chip. They are not to be confused with the bias of a neuron in a neural network.\n",
    "\n",
    "After the parameters have been set, all neurons should be quiet. Sometimes there are \"hot\" neurons, which keep firing anyway. We will identify those and simply disable them.\n",
    "\n",
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Imports and parameters\n",
    "from rockpool.devices import rectangular_neuron_arrangement, DynapseControlExtd\n",
    "from rockpool.layers import RecDynapSE\n",
    "\n",
    "# Path for loading circuit biases (which define neuron and synapse characteristics)\n",
    "bias_path = \"network/biases.py\"\n",
    "\n",
    "# How long to scan for 'hot' neurons that fire spontaneously\n",
    "silence_hot_neurons_dur = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron arrangement\n",
    "\n",
    "We can choose explicitely to which individual neurons on the chip the neurons from the network are mapped to. It makes sense to put different partitions of the reservoir (input expansion, excitatory, inhibitory) on different cores, so that the neuron dynamics can be adjusted individually.\n",
    "\n",
    "Furthermore, the neurons will be assigned so they form rectangles. This makes it easier to visually identify individual neurons in cortexcontrol, the software interface to the chip.\n",
    "\n",
    "We also need to select virtual neurons, that act as a source for the external spikes from the signal-to-spike layer. The important thing is that they should have different IDs than the hardware neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Reservoir neuron arangement\n",
    "rectangular_arrangement = [\n",
    "    # Input layer\n",
    "    {\"first_neuron\": 4, \"num_neurons\": size_in, \"width\": 8},\n",
    "    # Reservoir layer I\n",
    "    {\"first_neuron\": 256, \"num_neurons\": 256, \"width\": 16},\n",
    "    # Reservoir layer II\n",
    "    {\"first_neuron\": 768, \"num_neurons\": size_rec - 256, \"width\": 16},\n",
    "    # Inhibitory layer\n",
    "    {\"first_neuron\": 516, \"num_neurons\": size_inhib, \"width\": 8},\n",
    "]\n",
    "\n",
    "neuron_ids = []\n",
    "for rectangle_params in rectangular_arrangement:\n",
    "    neuron_ids += list(rectangular_neuron_arrangement(**rectangle_params))\n",
    "    \n",
    "# - 'Virtual' neurons (input neurons)\n",
    "virtual_neuron_ids = [1, 2, 3, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynapse_control: RPyC connection established through port 1300.\n",
      "dynapse_control: RPyC namespace complete.\n",
      "dynapse_control: RPyC connection has been setup successfully.\n",
      "DynapseControl: Initializing DynapSE\n",
      "DynapseControl: Spike generator module ready.\n",
      "DynapseControl: Poisson generator module ready.\n",
      "DynapseControl: Time constants of cores [] have been reset.\n",
      "DynapseControl: Neurons initialized.\n",
      "\t 0 hardware neurons and 1023 virtual neurons available.\n",
      "DynapseControl: Neuron connector initialized\n",
      "DynapseControl: Connectivity array initialized\n",
      "DynapseControl: FPGA spike generator prepared.\n",
      "DynapseControl ready.\n",
      "DynapseControl: Biases have been loaded from biases.py.\n",
      "DynapseControl: Collecting IDs of neurons that spike within the next 5 seconds\n",
      "DynapseControl: Generated new buffered event filter.\n",
      "DynapseControl: 28 neurons spiked: [293, 302, 303, 335, 355, 364, 420, 428, 453, 523, 536, 570, 585, 601, 618, 681, 709, 761, 825, 836, 851, 853, 910, 942, 953, 986, 990, 1009]\n",
      "DynapseControl: Neurons [293, 302, 303, 335, 355, 364, 420, 428, 453, 523, 536, 570, 585, 601, 618, 681, 709, 761, 825, 836, 851, 853, 910, 942, 953, 986, 990, 1009] will be silenced\n",
      "DynapseControl: 28 neurons have been silenced.\n"
     ]
    }
   ],
   "source": [
    "# - Set up DynapseControl\n",
    "\n",
    "controller = DynapseControlExtd(fpga_isibase=reservoir.dt)\n",
    "\n",
    "# Circuit biases\n",
    "controller.load_biases(bias_path)\n",
    "\n",
    "# Silence 'hot' neurons that fire continuously\n",
    "hot_neurons = controller.silence_hot_neurons(neuron_ids, silence_hot_neurons_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecDynapSE `hardware`: Superclass initialized\n",
      "DynapseControl: Connections to cores [0 1 2 3] have been cleared.\n",
      "DynapseControl: For some of the requested neurons, chips need to be prepared.\n",
      "dynapse_control: Chips 0 have been cleared.\n",
      "DynapseControl: 1023 hardware neurons available.\n",
      "Layer `hardware`: Layer neurons allocated\n",
      "Layer `hardware`: Virtual neurons allocated\n",
      "DynapseControl: Excitatory connections of type `FAST_EXC` between virtual and hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between virtual and hardware neurons have been set.\n",
      "Layer `hardware`: Connections to virtual neurons have been set.\n",
      "DynapseControl: Excitatory connections of type `FAST_EXC` between hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between hardware neurons have been set.\n",
      "Layer `hardware`: Connections from input neurons to reservoir have been set.\n",
      "DynapseControl: Excitatory connections of type `SLOW_EXC` between hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between hardware neurons have been set.\n",
      "DynapseControl: Connections have been written to the chip.\n",
      "Layer `hardware`: Recurrent connections have been set.\n",
      "Layer `hardware` prepared.\n"
     ]
    }
   ],
   "source": [
    "# - Set up hardware reservoir layer\n",
    "hw_layer = RecDynapSE(\n",
    "    weights_in=weights_res_in,\n",
    "    weights_rec=weights_rec,\n",
    "    neuron_ids=neuron_ids,\n",
    "    virtual_neuron_ids=virtual_neuron_ids,\n",
    "    dt=reservoir.dt,\n",
    "    controller=controller,\n",
    "    clearcores_list=[0,1,2,3],\n",
    "    name=\"hardware\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Separate readout layer (for different readout weights)\n",
    "readout_hw = FFExpSyn(\n",
    "    weights=np.zeros((size_full, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_net.reset_all()\n",
    "hw_net = Network(spike_enc, hw_layer, readout_hw, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Generator that yields batches of ECG data\n",
    "# batch_gen = data_loader.get_batch_generator(num_beats=100, batchsize=batchsize_training)\n",
    "\n",
    "# for batch in batch_gen:\n",
    "#     output = hw_net.evolve(batch.input)\n",
    "#     readout_hw.train_rr(\n",
    "#         batch.target,\n",
    "#         output[\"hardware\"],\n",
    "#         is_first=batch.is_first,\n",
    "#         is_last=batch.is_last,\n",
    "#         regularize=regularize,\n",
    "#     )\n",
    "# hw_net.reset_all()\n",
    "\n",
    "# np.save(\"weights/readout_weights_hw.py\", readout_hw.weights)\n",
    "# np.save(\"weights/readout_bias_hw.py\", readout_hw.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load pre-trained weights\n",
    "readout_hw.weights = np.load(\"network/readout_weights_hw.py\")\n",
    "readout_hw.bias = np.load(\"network/readout_bias_hw.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "num_beats = 100\n",
    "ecg_data = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "net_data = hw_net.evolve(ecg_data.input)\n",
    "output = net_data[\"readout\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "target = ecg_data.target\n",
    "any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(8, 10))\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    top=0.98, bottom=0.05, left=0.15, right=0.95, hspace=0.5, wspace=0.2\n",
    ")\n",
    "\n",
    "for i_anom, (ax, lbl) in enumerate(zip(axes.flatten()[:-1], labels)):\n",
    "    output.clip(channels=i_anom).plot(target=ax)\n",
    "    any_target.plot(target=ax, color=\"gray\", alpha=0.5)\n",
    "    target.clip(channels=i_anom).plot(target=ax)\n",
    "    ax.set_title(lbl)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
