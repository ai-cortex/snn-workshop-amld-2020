{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware demonstration: ECG anomaly detection\n",
    "\n",
    "Author: Felix Bauer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how recurrent SNNs can be used for anomaly detection in an electrocardiogram (ECG) signal. The main part of the network will run on a DYNAP-SE neuromorphic processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Our goal is to detect five different classes of anomalies in a two-lead ECG signal from the MIT-BIH Arrithmia Database [ref]. An SNN that fulfills this task can, for instance, be used in a wearable ECG monitoring device to trigger an alarm in presence of pathological patterns. Below we see examples for a normal ECG signal and for each anomaly type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /home/felix/miniconda3/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from scripts.plot_example_beats import plot_examples, labels\n",
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware\n",
    "\n",
    "We will first run a software simulation of our SNN and then run the network directly on a DYNAP-SE neuromorphic processor. The device we are demonstrating here is a prototype that imposes a few restrictions on the network, which are described below and which will also be considered in the software simulations.\n",
    "\n",
    "## Core-wise parameters\n",
    "Neuron and synapse parameters, such as time constants, firing thresholds and weights are set per core. The present processor consists of 16 cores of 256 neurons each. \n",
    "\n",
    "## Discrete weights\n",
    "Synaptic weights are the same for each postsynaptic neuron on a core, resulting in ternary weights: positive (excitatory), negative (inhibitory), and zero (not connected). However, between each pair of neurons, multiple connections are possible, which effectively allows for integer weights.\n",
    "\n",
    "## Connectivity\n",
    "The number of presynaptic connections to each neuron is generally limited to 64.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "To load the ECG data we will use a data loader class that is specifically written for this purpose. You find its source code in the folder of this tutorial. \n",
    "The ECG data itself is distributed on [PhysioNet](http://physionet.org/physiobank/database/mitdb/). We extracted the signal and its annotations to a .npy-file and a .csv file that can be downloaded from [here](https://www.dropbox.com/s/ocqo8oog0xv4qrm/ecg_data.zip?dl=1). If you want to run this notebook you need to extract the files and save them in the folder called `ecg_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG signal and annotaitions have been loaded from /home/felix/gitlab/amld-2020-aictx/Notebooks/ECG_demo/ecg_data\n"
     ]
    }
   ],
   "source": [
    "from scripts.dataloader import ECGDataLoader\n",
    "\n",
    "# - Object to load ECG data\n",
    "data_loader = ECGDataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal-to-spike encoding\n",
    "\n",
    "The analog ECG signal is converted to trains of events through a sigma-delta encoding scheme. For every ECG lead there are two output channels, emitting spikes when the input signal increases (\"up\"-channel) or decreases (\"down\"-channel) by a specified amount.\n",
    "\n",
    "The four resulting spike trains serve as input to the acutal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import FFUpDown\n",
    "\n",
    "# - ECG signal parameters\n",
    "DT_ECG = 0.002778\n",
    "NUM_ECG_LEADS = 2\n",
    "NUM_ANOM_CLASSES = 4\n",
    "\n",
    "# - Spike encoding\n",
    "spike_enc = FFUpDown(\n",
    "    weights=NUM_ECG_LEADS,\n",
    "    dt=DT_ECG,    \n",
    "    thr_up=0.1,\n",
    "    thr_down=0.1,\n",
    "    multiplex_spikes=True,\n",
    "    name=\"spike_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture\n",
    "\n",
    "\n",
    "To detect the anomalies we will use a partitioned reservoir network, consisting of 768 neurons. Each neuron is either excitatory or inhibitory. This is not strictly necessary but this way the different reservoir partitions can be placed onto different cores of the processor, which makes it easier to control the neuron dynamics.\n",
    "\n",
    "The weight matrices for the hardware will have integer values, corresponding to the number of connections between each pair of neurons. For the software simulation we will use the same matrix but scaled, such that the weights have the right strength. Different partitions of the reservoir will get different weights. For the hardware reservoir the weights will be scaled later on by setting a hardware parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /home/felix/miniconda3/lib/python3.7/site-packages/nest/lib/hl_api_helper.py:127: UserWarning:\n",
      "ResetNetwork is deprecated and will be removed in NEST 3.0.\n",
      " [py.warnings]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rockpool.layers import RecIAFSpkInNest\n",
    "\n",
    "# Load weights\n",
    "weights_res_in = np.load(\"network/weights_res_in.npy\")\n",
    "weights_rec = np.load(\"network/weights_rec.npy\")\n",
    "\n",
    "# Scale weights for software simulation\n",
    "start_rec = 128\n",
    "start_inh = 128 + 512\n",
    "\n",
    "baseweight_inp = 5e-4\n",
    "baseweight_exp_rec = 8e-5\n",
    "baseweight_rec = 8e-5  # 1.75e-4\n",
    "baseweight_rec_inh = 8e-5\n",
    "baseweight_inh = 1e-4\n",
    "\n",
    "weights_res_in_scaled = weights_res_in.copy() * baseweight_inp\n",
    "\n",
    "weights_rec_scaled = weights_rec.copy()\n",
    "weights_rec_scaled[:start_rec, start_rec: start_inh] *= baseweight_exp_rec\n",
    "weights_rec_scaled[start_rec: start_inh, start_rec: start_inh] *= baseweight_rec\n",
    "weights_rec_scaled[start_rec: start_inh, start_inh:] *= baseweight_rec_inh\n",
    "weights_rec_scaled[start_inh:, start_rec: start_inh] *= baseweight_inh\n",
    "\n",
    "# - Load reservoir parameters from file (generated with gen_params.py)\n",
    "kwargs_reservoir = dict(np.load(\"network/kwargs_reservoir.npz\"))\n",
    "\n",
    "# - Instantiate reservoir layer object\n",
    "reservoir = RecIAFSpkInNest(\n",
    "    weights_in=weights_res_in_scaled,\n",
    "    weights_rec=weights_rec_scaled,\n",
    "    name=\"reservoir\",\n",
    "    **kwargs_reservoir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout layer\n",
    "\n",
    "The readout layer low-pass filters the reservoir spike trains to obtain an analog signal. It is then trained by ridge regression to perform a linear separation between the ECG anomaly types. There is one readout unit for each anomaly and the corresponding target is 1 whenever the anomaly is present and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import FFExpSyn\n",
    "\n",
    "readout = FFExpSyn(\n",
    "    weights=np.zeros((reservoir.size, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool import Network\n",
    "\n",
    "# - Network that holds the layers\n",
    "sw_net = Network(spike_enc, reservoir, readout, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In the following we will train the (software) readout layer. For this we generate batches of ECG data and evolve the network with it as input. After each batch the readout weights are updated. Although the linear regression algorithm is traditionally not for classification tasks, we use it here because it is very fast and efficient.\n",
    "\n",
    "_(We have trained the readout beforehand and will simply load the weights)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # - Generator that yields batches of ECG data\n",
    "# batchsize_training = 1000\n",
    "# num_beats = 15000\n",
    "# regularize = 0.1\n",
    "# batch_gen = data_loader.get_batch_generator(num_beats=num_beats, batchsize=batchsize_training)\n",
    "\n",
    "# t_start = time.time()\n",
    "# for batch in batch_gen:\n",
    "#     output = sw_net.evolve(batch.input)\n",
    "#     readout.train_rr(\n",
    "#         batch.target,\n",
    "#         output[\"reservoir\"],\n",
    "#         is_first=batch.is_first,\n",
    "#         is_last=batch.is_last,\n",
    "#         regularize=regularize,\n",
    "#     )\n",
    "    \n",
    "# sw_net.reset_all()\n",
    "# print(f\"Trained network in {time.time() - t_start:.2f} seconds.\")\n",
    "\n",
    "# np.save(\"network/readout_weights.npy\", readout.weights)\n",
    "# np.save(\"network/readout_bias.npy\", readout.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Test on training data\n",
    "# target = batch.target.start_at_zero()\n",
    "# res_data = output[\"reservoir\"]\n",
    "\n",
    "# test_on_training = readout.evolve(res_data.start_at_zero())\n",
    "# readout.reset_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# from rockpool import TSContinuous\n",
    "# test_on_training.clip(channels=3).plot()\n",
    "# target.clip(channels=3).plot()\n",
    "# any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "# any_target.plot(color=\"gray\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout.weights = np.load(\"network/readout_weights.npy\")\n",
    "readout.bias = np.load(\"network/readout_bias.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can now test our network to see how it performs with data it has not been trained on.\n",
    "The plots below show the output of each readout unit (blue). The targets are plotted in orange. \n",
    "\n",
    "Because the readout units are only trained to distinguish between normal and one specific anomaly, there many cross-detections. We therefore also plot a gray curve that indicates the presence of _any_ anomaly. For many applications it is sufficient to know that there is an anomaly. If one needs to classify which type it is, one could, for instnace, train an all-vs-all classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: Evolving layer `spike_encoder` with external input as input\n",
      "Network: Evolving layer `reservoir` with spike_encoder's output as input\n",
      "Network: Evolving layer `readout` with reservoir's output as input\n"
     ]
    }
   ],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "num_beats = 100\n",
    "ecg_data = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "net_data = sw_net.evolve(ecg_data.input)\n",
    "output = net_data[\"readout\"]\n",
    "sw_net.reset_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt\n",
    "from rockpool import TSContinuous\n",
    "\n",
    "target = ecg_data.target\n",
    "any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, bottom=0.05, left=0.05, right=0.95, hspace=0.2, wspace=0.2\n",
    ")\n",
    "\n",
    "for i_anom, (ax, lbl) in enumerate(zip(axes.flatten(), labels[1:])):\n",
    "    output.clip(channels=i_anom).plot(target=ax)\n",
    "    any_target.plot(target=ax, color=\"gray\", alpha=0.5)\n",
    "    target.clip(channels=i_anom).plot(target=ax)\n",
    "    ax.set_title(lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Implementation\n",
    "\n",
    "Now it is time to replace the software reservoir with the neuromorphic processor. As weights, we can simply use the (unscaled) integer weights from which we also generated the weights of the software reservoir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron arrangement\n",
    "\n",
    "We can choose explicitely to which individual neurons on the chip the neurons from the network are mapped to. It makes sense to put different partitions of the reservoir (input expansion, excitatory, inhibitory) on different cores, so that the neuron dynamics can be adjusted individually.\n",
    "\n",
    "Furthermore, the neurons will be assigned so they form rectangles. This makes it easier to visually identify individual neurons in cortexcontrol, the software interface to the chip.\n",
    "\n",
    "We also need to select virtual neurons, that act as a source for the external spikes from the signal-to-spike layer. The important thing is that they should have different IDs than the hardware neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.devices import rectangular_neuron_arrangement\n",
    "\n",
    "# - Reservoir neuron arangement\n",
    "rectangular_arrangement = [\n",
    "    # Input layer\n",
    "    {\"first_neuron\": 4, \"num_neurons\": 128, \"width\": 8},\n",
    "    # Reservoir layer I\n",
    "    {\"first_neuron\": 256, \"num_neurons\": 256, \"width\": 16},\n",
    "    # Reservoir layer II\n",
    "    {\"first_neuron\": 768, \"num_neurons\": 512 - 256, \"width\": 16},\n",
    "    # Inhibitory layer\n",
    "    {\"first_neuron\": 516, \"num_neurons\": 128, \"width\": 8},\n",
    "]\n",
    "\n",
    "neuron_ids = []\n",
    "for rectangle_params in rectangular_arrangement:\n",
    "    neuron_ids += list(rectangular_neuron_arrangement(**rectangle_params))\n",
    "    \n",
    "# - 'Virtual' neurons (input neurons)\n",
    "virtual_neuron_ids = [1, 2, 3, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller class\n",
    "Rockpool's `DynapseControl` and `DynapseControlExtd` classes allow convenient interfacing with the hardware.\n",
    "\n",
    "We will now load neuron and synapse parameters from a file directly onto the chip. They are chosen so that the resulting dynamics are close to that of the simulation.\n",
    "\n",
    "The hardware parameters are often refered to as \"biases\", because they correspond to biases in circuits on the chip. They are not to be confused with the bias of a neuron in a neural network.\n",
    "\n",
    "After the parameters have been set, all neurons should be quiet. Sometimes there are \"hot\" neurons, which fire spontaneously. We will identify those and disable them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynapse_control: RPyC connection established through port 1300.\n",
      "dynapse_control: Already initialized chips: [0]\n",
      "dynapse_control: RPyC namespace complete.\n",
      "dynapse_control: RPyC connection has been setup successfully.\n",
      "DynapseControl: Initializing DynapSE\n",
      "DynapseControl: Spike generator module ready.\n",
      "DynapseControl: Poisson generator module ready.\n",
      "DynapseControl: Time constants of cores [0 1 2 3] have been reset.\n",
      "DynapseControl: Neurons initialized.\n",
      "\t 1023 hardware neurons and 1023 virtual neurons available.\n",
      "DynapseControl: Neuron connector initialized\n",
      "DynapseControl: Connectivity array initialized\n",
      "DynapseControl: FPGA spike generator prepared.\n",
      "DynapseControl ready.\n",
      "DynapseControl: Biases have been loaded from network/biases.py.\n",
      "DynapseControl: Collecting IDs of neurons that spike within the next 5 seconds\n",
      "DynapseControl: Generated new buffered event filter.\n",
      "DynapseControl: 18 neurons spiked: [302, 303, 400, 420, 453, 536, 585, 601, 618, 681, 696, 825, 851, 942, 953, 986, 1009, 1017]\n",
      "DynapseControl: Neurons [302, 303, 400, 420, 453, 536, 585, 601, 618, 681, 696, 825, 851, 942, 953, 986, 1009, 1017] will be silenced\n",
      "DynapseControl: 18 neurons have been silenced.\n"
     ]
    }
   ],
   "source": [
    "from rockpool.devices import DynapseControlExtd\n",
    "\n",
    "# - Set up DynapseControl\n",
    "controller = DynapseControlExtd()\n",
    "\n",
    "# - Load circuit biases (which define neuron and synapse characteristics)\n",
    "bias_path = \"network/biases.py\"\n",
    "controller.load_biases(bias_path)\n",
    "\n",
    "# Silence 'hot' neurons that fire spontaneously\n",
    "silence_hot_neurons_dur = 5  # Time in seconds to listen if there are hot neurons\n",
    "hot_neurons = controller.silence_hot_neurons(neuron_ids, silence_hot_neurons_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware reservior class\n",
    "There is a rockpool layer, `RecDynapSE`, that directly sets up a reservoir on the hardware. It can be used just like the regular layers.\n",
    "\n",
    "The timestep `dt_hardware` determines the temporal resolution of data sent to the chip but does not affect computation time. Therefore we can give it a lower value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecDynapSE `hardware`: Superclass initialized\n",
      "DynapseControl: Connections to cores [0 1 2 3] have been cleared.\n",
      "Layer `hardware`: Layer neurons allocated\n",
      "Layer `hardware`: Virtual neurons allocated\n",
      "DynapseControl: Excitatory connections of type `FAST_EXC` between virtual and hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between virtual and hardware neurons have been set.\n",
      "Layer `hardware`: Connections to virtual neurons have been set.\n",
      "DynapseControl: Excitatory connections of type `FAST_EXC` between hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between hardware neurons have been set.\n",
      "Layer `hardware`: Connections from input neurons to reservoir have been set.\n",
      "DynapseControl: Excitatory connections of type `SLOW_EXC` between hardware neurons have been set.\n",
      "DynapseControl: Inhibitory connections of type `FAST_INH` between hardware neurons have been set.\n",
      "DynapseControl: Connections have been written to the chip.\n",
      "Layer `hardware`: Recurrent connections have been set.\n",
      "Layer `hardware` prepared.\n"
     ]
    }
   ],
   "source": [
    "from rockpool.layers import RecDynapSE\n",
    "\n",
    "# Hardware timestep\n",
    "dt_hardware = 0.1 * reservoir.dt\n",
    "\n",
    "# - Set up hardware reservoir layer\n",
    "reservoir_hw = RecDynapSE(\n",
    "    weights_in=weights_res_in,\n",
    "    weights_rec=weights_rec,\n",
    "    neuron_ids=neuron_ids,\n",
    "    virtual_neuron_ids=virtual_neuron_ids,\n",
    "    dt=dt_hardware,\n",
    "    controller=controller,\n",
    "    clearcores_list=[0,1,2,3],\n",
    "    name=\"hardware\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout and spike encoder\n",
    "\n",
    "The readout and signal-to-spike-encoding layer can be exactly the same as for the software simulation. We will create an identical copy of the software readout so that they can have different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Separate readout layer for hardware (for different readout weights)\n",
    "readout_hw = FFExpSyn(\n",
    "    weights=np.zeros((reservoir_hw.size, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_net.reset_all()\n",
    "hw_net = Network(spike_enc, reservoir_hw, readout_hw, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training and inference work the same way as with the software layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Generator that yields batches of ECG data\n",
    "# batch_gen = data_loader.get_batch_generator(num_beats=100, batchsize=batchsize_training)\n",
    "\n",
    "# for batch in batch_gen:\n",
    "#     output = hw_net.evolve(batch.input)\n",
    "#     readout_hw.train_rr(\n",
    "#         batch.target,\n",
    "#         output[\"hardware\"],\n",
    "#         is_first=batch.is_first,\n",
    "#         is_last=batch.is_last,\n",
    "#         regularize=regularize,\n",
    "#     )\n",
    "# hw_net.reset_all()\n",
    "\n",
    "# np.save(\"weights/readout_weights_hw.py\", readout_hw.weights)\n",
    "# np.save(\"weights/readout_bias_hw.py\", readout_hw.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load pre-trained weights\n",
    "readout_hw.weights = np.load(\"network/readout_weights_hw.npy\")[:, :4]\n",
    "readout_hw.bias = np.load(\"network/readout_bias_hw.npy\")[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: Evolving layer `spike_encoder` with external input as input\n",
      "Network: Evolving layer `hardware` with spike_encoder's output as input\n",
      "Layer `hardware`: Current batch input: 88.662648 s (638320 timesteps), 8760 events, from 0.0 s to 88.662648 s of 88.662648 s\n",
      "DynapseControl: Generating FPGA event list from arrays.\n",
      "DynapseControl: Stimulus prepared from arrays.\n",
      "DynapseControl: Stimulus preloaded.\n",
      "DynapseControl: Updated existing buffered event filter.\n",
      "DynapseControl: Started stimulation for 88.662648 s.\n",
      "\tRecorded 2887365 event(s) and 1 trigger event(s)\n",
      "\t\t Using trigger event 0\n",
      "DynapseControl: Extracted event data\n",
      "Layer `hardware`: Received event data\n",
      "Layer `hardware`: Evolution successful.\n",
      "Network: Evolving layer `readout` with hardware's output as input\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "num_beats = 100\n",
    "ecg_data = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "ecg_data.input.plot()\n",
    "\n",
    "net_data = hw_net.evolve(ecg_data.input)\n",
    "hw_net.reset_all()\n",
    "output = net_data[\"readout\"]\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f3984bf15d4d58a4283548b757db17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "target = ecg_data.target\n",
    "any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, bottom=0.05, left=0.15, right=0.95, hspace=0.5, wspace=0.2\n",
    ")\n",
    "\n",
    "for i_anom, (ax, lbl) in enumerate(zip(axes.flatten(), labels[1:])):\n",
    "    output.clip(channels=i_anom).plot(target=ax)\n",
    "    any_target.plot(target=ax, color=\"gray\", alpha=0.5)\n",
    "    target.clip(channels=i_anom).plot(target=ax)\n",
    "    ax.set_title(lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
