{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware demonstration: ECG anomaly detection\n",
    "\n",
    "Author: Felix Bauer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how recurrent SNNs can be used for anomaly detection in an electrocardiogram (ECG) signal. The main part of the network will run on a DYNAP-SE neuromorphic processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Our goal is to detect four different classes of anomalies in a two-lead ECG signal from the MIT-BIH Arrithmia Database (*), which is distributed on [PhysioNet](http://physionet.org/physiobank/database/mitdb/). An SNN that fulfills this task can, for instance, be used in a wearable ECG monitoring device to trigger an alarm in presence of pathological patterns. Below we see examples for a normal ECG signal and for each anomaly type."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(*):\n",
    "    Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals (2003). Circulation. 101(23):e215-e220."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec('ipympl') is None:\n",
    "  !pip install ipympl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# - Disable warning display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scripts.plot_example_beats import plot_examples, labels\n",
    "plot_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware\n",
    "\n",
    "We will first run a software simulation of our SNN and then run the network directly on a DYNAP-SE neuromorphic processor. The device we are demonstrating here is a prototype that imposes a few restrictions on the network, which are described below and which will also be considered in the software simulations.\n",
    "\n",
    "## Core-wise parameters\n",
    "Neuron and synapse parameters, such as time constants, firing thresholds and weights are set per core. The present processor consists of 16 cores of 256 neurons each. \n",
    "\n",
    "## Discrete weights\n",
    "Synaptic weights are the same for each postsynaptic neuron on a core, resulting in ternary weights: positive (excitatory), negative (inhibitory), and zero (not connected). However, between each pair of neurons, multiple connections are possible, which effectively allows for integer weights.\n",
    "\n",
    "## Connectivity\n",
    "The number of presynaptic connections to each neuron is generally limited to 64.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "To load the ECG data we will use a data loader class that is specifically written for this purpose. You find its source code in the folder of this tutorial. \n",
    "The ECG data itself is distributed on [PhysioNet](http://physionet.org/physiobank/database/mitdb/). We extracted the signal and its annotations to a .npy-file and a .csv file that can be downloaded from [here](https://www.dropbox.com/s/ocqo8oog0xv4qrm/ecg_data.zip?dl=1). If you want to run this notebook you need to extract the files and save them in the folder called `ecg_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dataloader import ECGDataLoader\n",
    "\n",
    "# - Object to load ECG data\n",
    "data_loader = ECGDataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal-to-spike encoding\n",
    "\n",
    "The analog ECG signal is converted to trains of events through a sigma-delta encoding scheme. For every ECG lead there are two output channels, emitting spikes when the input signal increases (\"up\"-channel) or decreases (\"down\"-channel) by a specified amount.\n",
    "\n",
    "The four resulting spike trains serve as input to the acutal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import FFUpDown\n",
    "\n",
    "# - ECG signal parameters\n",
    "DT_ECG = 0.002778\n",
    "NUM_ECG_LEADS = 2\n",
    "NUM_ANOM_CLASSES = 4\n",
    "\n",
    "# - Spike encoding\n",
    "spike_enc = FFUpDown(\n",
    "    weights=NUM_ECG_LEADS,\n",
    "    dt=DT_ECG,    \n",
    "    thr_up=0.1,\n",
    "    thr_down=0.1,\n",
    "    multiplex_spikes=True,\n",
    "    name=\"spike_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture\n",
    "\n",
    "\n",
    "To detect the anomalies we will use a partitioned reservoir network, consisting of 768 neurons. Each neuron is either excitatory or inhibitory. This is not strictly necessary but this way the different reservoir partitions can be placed onto different cores of the processor, which makes it easier to control the neuron dynamics. Another modification from the standard random reservoir architecture is that we will include an input expansion layer before the actual reservoir. This feedforward layer helps to increase the dimensionality of the signal and makes it easier on the hardware to scale the input to the reservoir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1eDVCV5un7VK0vxM8VEd53NKazMXHvH3J\" width=65% /><br />\n",
    "    <i> Partitioned reservoir with input expansion layer </i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrices for the hardware will have integer values, corresponding to the number of connections between each pair of neurons. For the software simulation we will use the same matrix but scaled, such that the weights have the right strength. Different partitions of the reservoir will get different weights. For the hardware reservoir the weights will be scaled later on by setting a hardware parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from rockpool.layers import RecIAFSpkInNest\n",
    "\n",
    "# Load weights\n",
    "weights_res_in = np.load(\"network/weights_res_in.npy\")\n",
    "weights_rec = np.load(\"network/weights_rec.npy\")\n",
    "\n",
    "# Scale weights for software simulation\n",
    "start_rec = 128\n",
    "start_inh = 128 + 512\n",
    "\n",
    "baseweight_inp = 5e-4\n",
    "baseweight_exp_rec = 8e-5\n",
    "baseweight_rec = 8e-5  # 1.75e-4\n",
    "baseweight_rec_inh = 8e-5\n",
    "baseweight_inh = 1e-4\n",
    "\n",
    "weights_res_in_scaled = weights_res_in.copy() * baseweight_inp\n",
    "\n",
    "weights_rec_scaled = weights_rec.copy()\n",
    "weights_rec_scaled[:start_rec, start_rec: start_inh] *= baseweight_exp_rec\n",
    "weights_rec_scaled[start_rec: start_inh, start_rec: start_inh] *= baseweight_rec\n",
    "weights_rec_scaled[start_rec: start_inh, start_inh:] *= baseweight_rec_inh\n",
    "weights_rec_scaled[start_inh:, start_rec: start_inh] *= baseweight_inh\n",
    "\n",
    "# - Load reservoir parameters from file (generated with gen_params.py)\n",
    "kwargs_reservoir = dict(np.load(\"network/kwargs_reservoir.npz\"))\n",
    "\n",
    "# - Instantiate reservoir layer object\n",
    "reservoir = RecIAFSpkInNest(\n",
    "    weights_in=weights_res_in_scaled,\n",
    "    weights_rec=weights_rec_scaled,\n",
    "    name=\"reservoir\",\n",
    "    **kwargs_reservoir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout layer\n",
    "\n",
    "The readout layer low-pass filters the reservoir spike trains to obtain an analog signal. It is then trained by ridge regression to perform a linear separation between the ECG anomaly types. There is one readout unit for each anomaly and the corresponding target is 1 whenever the anomaly is present and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import FFExpSyn\n",
    "\n",
    "readout = FFExpSyn(\n",
    "    weights=np.zeros((reservoir.size, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool import Network\n",
    "\n",
    "# - Network that holds the layers\n",
    "sw_net = Network(spike_enc, reservoir, readout, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In the following we will train the (software) readout layer. For this we generate batches of ECG data and evolve the network with it as input. After each batch the readout weights are updated. Although the linear regression algorithm is traditionally not for classification tasks, we use it here because it is very fast and efficient.\n",
    "\n",
    "_(We have trained the readout beforehand and will simply load the weights)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # - Generator that yields batches of ECG data\n",
    "# batchsize_training = 1000\n",
    "# num_beats = 15000\n",
    "# regularize = 0.1\n",
    "# batch_gen = data_loader.get_batch_generator(num_beats=num_beats, batchsize=batchsize_training)\n",
    "\n",
    "# t_start = time.time()\n",
    "# for batch in batch_gen:\n",
    "#     output = sw_net.evolve(batch.input)\n",
    "#     readout.train_rr(\n",
    "#         batch.target,\n",
    "#         output[\"reservoir\"],\n",
    "#         is_first=batch.is_first,\n",
    "#         is_last=batch.is_last,\n",
    "#         regularize=regularize,\n",
    "#     )\n",
    "    \n",
    "# sw_net.reset_all()\n",
    "# print(f\"Trained network in {time.time() - t_start:.2f} seconds.\")\n",
    "\n",
    "# np.save(\"network/readout_weights.npy\", readout.weights)\n",
    "# np.save(\"network/readout_bias.npy\", readout.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Test on training data\n",
    "# target = batch.target.start_at_zero()\n",
    "# res_data = output[\"reservoir\"]\n",
    "\n",
    "# test_on_training = readout.evolve(res_data.start_at_zero())\n",
    "# readout.reset_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# from rockpool import TSContinuous\n",
    "# test_on_training.clip(channels=3).plot()\n",
    "# target.clip(channels=3).plot()\n",
    "# any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "# any_target.plot(color=\"gray\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout.weights = np.load(\"network/readout_weights.npy\")\n",
    "readout.bias = np.load(\"network/readout_bias.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can now test our network to see how it performs with data it has not been trained on.\n",
    "The plots below show the output of each readout unit (blue). The targets are plotted in orange. \n",
    "\n",
    "Because the readout units are only trained to distinguish between normal and one specific anomaly, there many cross-detections. We therefore also plot a gray curve that indicates the presence of _any_ anomaly. For many applications it is sufficient to know that there is an anomaly. If one needs to classify which type it is, one could, for instnace, train an all-vs-all classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "num_beats = 100\n",
    "ecg_data = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "net_data = sw_net.evolve(ecg_data.input)\n",
    "output = net_data[\"readout\"]\n",
    "sw_net.reset_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt\n",
    "from rockpool import TSContinuous\n",
    "\n",
    "target = ecg_data.target.copy()\n",
    "any_target = TSContinuous(target.times, np.any(target.samples, axis=1))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, bottom=0.05, left=0.05, right=0.95, hspace=0.2, wspace=0.2\n",
    ")\n",
    "\n",
    "for i_anom, (ax, lbl) in enumerate(zip(axes.flatten(), labels[1:])):\n",
    "    output.clip(channels=i_anom).plot(target=ax)\n",
    "    any_target.plot(target=ax, color=\"gray\", alpha=0.5)\n",
    "    target.clip(channels=i_anom).plot(target=ax)\n",
    "    ax.set_title(lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Implementation\n",
    "\n",
    "Now it is time to replace the software reservoir with the neuromorphic processor. As weights, we can simply use the (unscaled) integer weights from which we also generated the weights of the software reservoir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron arrangement\n",
    "\n",
    "We can choose explicitely to which individual neurons on the chip the neurons from the network are mapped to. It makes sense to put different partitions of the reservoir (input expansion, excitatory, inhibitory) on different cores, so that the neuron dynamics can be adjusted individually.\n",
    "\n",
    "Furthermore, the neurons will be assigned so they form rectangles. This makes it easier to visually identify individual neurons in cortexcontrol, the software interface to the chip.\n",
    "\n",
    "We also need to select virtual neurons, that act as a source for the external spikes from the signal-to-spike layer. The important thing is that they should have different IDs than the hardware neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.devices import rectangular_neuron_arrangement\n",
    "\n",
    "# - Reservoir neuron arangement\n",
    "rectangular_arrangement = [\n",
    "    # Input layer\n",
    "    {\"first_neuron\": 4, \"num_neurons\": 128, \"width\": 8},\n",
    "    # Reservoir layer I\n",
    "    {\"first_neuron\": 256, \"num_neurons\": 256, \"width\": 16},\n",
    "    # Reservoir layer II\n",
    "    {\"first_neuron\": 768, \"num_neurons\": 512 - 256, \"width\": 16},\n",
    "    # Inhibitory layer\n",
    "    {\"first_neuron\": 516, \"num_neurons\": 128, \"width\": 8},\n",
    "]\n",
    "\n",
    "neuron_ids = []\n",
    "for rectangle_params in rectangular_arrangement:\n",
    "    neuron_ids += list(rectangular_neuron_arrangement(**rectangle_params))\n",
    "    \n",
    "# - 'Virtual' neurons (input neurons)\n",
    "virtual_neuron_ids = [1, 2, 3, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller class\n",
    "Rockpool's `DynapseControl` and `DynapseControlExtd` classes allow convenient interfacing with the hardware.\n",
    "\n",
    "We will now load neuron and synapse parameters from a file directly onto the chip. They are chosen so that the resulting dynamics are close to that of the simulation.\n",
    "\n",
    "The hardware parameters are often refered to as \"biases\", because they correspond to biases in circuits on the chip. They are not to be confused with the bias of a neuron in a neural network.\n",
    "\n",
    "After the parameters have been set, all neurons should be quiet. Sometimes there are \"hot\" neurons, which fire spontaneously. We will identify those and disable them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.devices import DynapseControlExtd\n",
    "\n",
    "# - Set up DynapseControl\n",
    "controller = DynapseControlExtd()\n",
    "\n",
    "# - Load circuit biases (which define neuron and synapse characteristics)\n",
    "bias_path = \"network/biases.py\"\n",
    "controller.load_biases(bias_path)\n",
    "\n",
    "# Silence 'hot' neurons that fire spontaneously\n",
    "silence_hot_neurons_dur = 5  # Time in seconds to listen if there are hot neurons\n",
    "hot_neurons = controller.silence_hot_neurons(neuron_ids, silence_hot_neurons_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware reservior class\n",
    "There is a rockpool layer, `RecDynapSE`, that directly sets up a reservoir on the hardware. It can be used just like the regular layers.\n",
    "\n",
    "The timestep `dt_hardware` determines the temporal resolution of data sent to the chip but does not affect computation time. Therefore we can give it a lower value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rockpool.layers import RecDynapSE\n",
    "\n",
    "# Hardware timestep\n",
    "dt_hardware = 0.1 * reservoir.dt\n",
    "\n",
    "# - Set up hardware reservoir layer\n",
    "reservoir_hw = RecDynapSE(\n",
    "    weights_in=weights_res_in,\n",
    "    weights_rec=weights_rec,\n",
    "    neuron_ids=neuron_ids,\n",
    "    virtual_neuron_ids=virtual_neuron_ids,\n",
    "    dt=dt_hardware,\n",
    "    controller=controller,\n",
    "    # clearcores_list=[0,1,2,3],\n",
    "    skip_weights=True,\n",
    "    name=\"hardware\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout and spike encoder\n",
    "\n",
    "The readout and signal-to-spike-encoding layer can be exactly the same as for the software simulation. We will create an identical copy of the software readout so that they can have different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Separate readout layer for hardware (for different readout weights)\n",
    "readout_hw = FFExpSyn(\n",
    "    weights=np.zeros((reservoir_hw.size, NUM_ANOM_CLASSES)),\n",
    "    bias=0,\n",
    "    dt=DT_ECG,\n",
    "    tau_syn=0.175,\n",
    "    name=\"readout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_net.reset_all()\n",
    "hw_net = Network(spike_enc, reservoir_hw, readout_hw, dt=DT_ECG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training and inference work the same way as with the software layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Generator that yields batches of ECG data\n",
    "# batch_gen = data_loader.get_batch_generator(num_beats=100, batchsize=batchsize_training)\n",
    "\n",
    "# for batch in batch_gen:\n",
    "#     output = hw_net.evolve(batch.input)\n",
    "#     readout_hw.train_rr(\n",
    "#         batch.target,\n",
    "#         output[\"hardware\"],\n",
    "#         is_first=batch.is_first,\n",
    "#         is_last=batch.is_last,\n",
    "#         regularize=regularize,\n",
    "#     )\n",
    "# hw_net.reset_all()\n",
    "\n",
    "# np.save(\"weights/readout_weights_hw.py\", readout_hw.weights)\n",
    "# np.save(\"weights/readout_bias_hw.py\", readout_hw.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load pre-trained weights\n",
    "readout_hw.weights = np.load(\"network/readout_weights_hw.npy\")[:, :4]\n",
    "readout_hw.bias = np.load(\"network/readout_bias_hw.npy\")[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Generator that yields batches of ECG data\n",
    "num_beats = 100\n",
    "ecg_data_hw = data_loader.get_single_batch(num_beats=num_beats)\n",
    "\n",
    "ecg_data_hw.input.plot()\n",
    "\n",
    "net_data_hw = hw_net.evolve(ecg_data_hw.input)\n",
    "hw_net.reset_all()\n",
    "output_hw = net_data_hw[\"readout\"]\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data at its different stages of processing:\n",
    "\n",
    "### Conversion into spikes\n",
    "\n",
    "Signal-to-spike conversion is the same as in software simulation. There are two ECG leads (channels). We use Sigma-Delta encoding, so the signal is encoded in four spike trains in total.\n",
    "\n",
    "### Reservoir activity\n",
    "\n",
    "For each partition of the reservoir the neurons exhibit distinct firing activities.\n",
    "\n",
    "- In the **input expansion layer (neurons 0-127)** neurons are directly activated by the input channels. There are four populations in this layer, each responding to one channel.\n",
    "- In the **excitatory layer (neurons 128-639)** the recurrent connectinos cause the neurons to keep firing even when there is currently no spiking input.\n",
    "- The **inhibitory layer's (neurons 640-767)** neurons are activated by the recurrently connected excitatory neurons. Therefore they also keep firing when there are no input spikes.\n",
    "\n",
    "### The readout\n",
    "The network outputs are weighted sums of the low-pass filtered reservoir spike trains, just like in the software simulation. Also here, cross detections might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# - For each processing stage plot a few seconds of its signal\n",
    "fig, axes = plt.subplots(4, sharex=True, figsize=(8,40))\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, bottom=0.05, left=0.15, right=0.95, hspace=0.5, wspace=0.2\n",
    ")\n",
    "\n",
    "t_start = 5\n",
    "t_stop = 7\n",
    "net_data_hw[\"readout\"] = abs(net_data_hw[\"readout\"])\n",
    "\n",
    "for layername, ax in zip([\"external\", \"spike_encoder\", \"hardware\", \"readout\"], axes):\n",
    "    if layername in [\"spike_encoder\", \"hardware\"]:\n",
    "        net_data_hw[layername].clip(t_start=5, t_stop=7).plot(target=ax, s=2)\n",
    "    else:\n",
    "        net_data_hw[layername].clip(t_start=5, t_stop=7).plot(target=ax)\n",
    "    ax.set_title(layername)\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "\n",
    "# - Plot target\n",
    "target_hw = ecg_data_hw.target.copy()\n",
    "any_target_hw = TSContinuous(target_hw.times, np.any(target_hw.samples, axis=1))\n",
    "any_target_hw.clip(t_start=t_start, t_stop=t_stop).plot(target=ax, color='k', lw=3, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    top=0.95, bottom=0.05, left=0.1, right=0.95, hspace=0.5, wspace=0.2\n",
    ")\n",
    "\n",
    "for i_anom, (ax, lbl) in enumerate(zip(axes.flatten(), labels[1:])):\n",
    "    abs(output_hw.clip(channels=i_anom)).plot(target=ax)\n",
    "    any_target_hw.plot(target=ax, color=\"limegreen\")\n",
    "    target_hw.clip(channels=i_anom).plot(target=ax)\n",
    "    ax.set_title(lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
